{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import csv\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score, precision_score, recall_score\n",
                "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
                "from sklearn.metrics import confusion_matrix, silhouette_score\n",
                "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score\n",
                "import argparse\n",
                "import os\n",
                "from collections import defaultdict\n",
                "from scipy import stats\n",
                "import warnings\n",
                "import pandas as pd\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "class Args:\n",
                "    path = './'\n",
                "    baseline_file = None\n",
                "    n_bootstrap = 1000\n",
                "    confidence_level = 0.95\n",
                "\n",
                "args = Args()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_ground_truth(json_file):\n",
                "    \"\"\"Loads ground truth labels from a JSON file.\"\"\"\n",
                "    with open(json_file, 'r') as f:\n",
                "        gt_data = json.load(f)\n",
                "    ap_to_floor = {int(k): v for k, v in gt_data.items()}\n",
                "    return ap_to_floor\n",
                "\n",
                "\n",
                "def load_clustering_result(csv_file):\n",
                "    \"\"\"Loads clustering results from a CSV file.\"\"\"\n",
                "    clusters = []\n",
                "    with open(csv_file, 'r') as f:\n",
                "        csv_reader = csv.reader(f)\n",
                "        for row in csv_reader:\n",
                "            cluster = [int(item.strip()) for item in row if item.strip() and item.strip().isdigit()]\n",
                "            if cluster:\n",
                "                clusters.append(cluster)\n",
                "    ap_to_cluster = {}\n",
                "    for cluster_id, aps in enumerate(clusters):\n",
                "        for ap in aps:\n",
                "            ap_to_cluster[ap] = cluster_id\n",
                "    return ap_to_cluster, clusters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def map_clusters_to_floors(ap_to_floor, ap_to_cluster, clusters, min_cluster_size=3):\n",
                "    \"\"\"\n",
                "    Kümeleri katlara eşler, ancak KÜÇÜK KÜMELERİ (-1) OLARAK ETİKETLER.\n",
                "    \"\"\"\n",
                "    cluster_floor_counts = defaultdict(lambda: defaultdict(int))\n",
                "    \n",
                "    common_aps = set(ap_to_floor.keys()) & set(ap_to_cluster.keys())\n",
                "    for ap in common_aps:\n",
                "        floor = ap_to_floor[ap]\n",
                "        cluster = ap_to_cluster[ap]\n",
                "        cluster_floor_counts[cluster][floor] += 1\n",
                "    \n",
                "    cluster_to_floor = {}\n",
                "    \n",
                "    # clusters listesi burada kullanılıyor\n",
                "    # clusters[cluster_id] bize o kümenin eleman listesini verir\n",
                "    for cluster_id, floor_counts in cluster_floor_counts.items():\n",
                "        \n",
                "        # Hata koruması: Eğer cluster_id liste sınırları dışındaysa (nadir durum)\n",
                "        if cluster_id >= len(clusters):\n",
                "            current_cluster_size = 0\n",
                "        else:\n",
                "            current_cluster_size = len(clusters[cluster_id])\n",
                "        \n",
                "        # --- KRİTİK KONTROL ---\n",
                "        if current_cluster_size < min_cluster_size:\n",
                "            cluster_to_floor[cluster_id] = -1  # GÜRÜLTÜ / NOISE olarak işaretle\n",
                "        else:\n",
                "            cluster_to_floor[cluster_id] = max(floor_counts.items(), key=lambda x: x[1])[0]\n",
                "            \n",
                "    return cluster_to_floor\n",
                "\n",
                "\n",
                "def create_true_pred_arrays(ap_to_floor, ap_to_cluster, cluster_to_floor):\n",
                "    \"\"\"Creates arrays of true and predicted labels for evaluation.\"\"\"\n",
                "    common_aps = sorted(set(ap_to_floor.keys()) & set(ap_to_cluster.keys()))\n",
                "    y_true = np.array([ap_to_floor[ap] for ap in common_aps])\n",
                "    y_pred_raw = np.array([ap_to_cluster[ap] for ap in common_aps])\n",
                "    y_pred_mapped = np.array([cluster_to_floor[ap_to_cluster[ap]] for ap in common_aps])\n",
                "    return common_aps, y_true, y_pred_raw, y_pred_mapped"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_cluster_quality_metrics(clusters, ap_to_floor):\n",
                "    \"\"\"\n",
                "    YENI FONKSİYON: Kümeleme kalitesini değerlendirir.\n",
                "    \n",
                "    Returns:\n",
                "        dict: Kümeleme kalite metrikleri\n",
                "    \"\"\"\n",
                "    total_aps = sum(len(cluster) for cluster in clusters)\n",
                "    num_clusters = len(clusters)\n",
                "    num_unique_floors = len(set(ap_to_floor.values()))\n",
                "    \n",
                "    # Cluster boyutları\n",
                "    cluster_sizes = [len(cluster) for cluster in clusters]\n",
                "    avg_cluster_size = np.mean(cluster_sizes)\n",
                "    std_cluster_size = np.std(cluster_sizes)\n",
                "    \n",
                "    # Singleton cluster sayısı (tek elemanlı küme)\n",
                "    singleton_clusters = sum(1 for size in cluster_sizes if size == 1)\n",
                "    singleton_ratio = singleton_clusters / num_clusters if num_clusters > 0 else 0\n",
                "    \n",
                "    # Küme içi homojenlik (her kümedeki dominant floor oranı)\n",
                "    cluster_purities = []\n",
                "    for cluster in clusters:\n",
                "        if not cluster:\n",
                "            continue\n",
                "        floor_counts = defaultdict(int)\n",
                "        valid_count = 0\n",
                "        for ap in cluster:\n",
                "            if ap in ap_to_floor:\n",
                "                floor_counts[ap_to_floor[ap]] += 1\n",
                "                valid_count += 1\n",
                "        if valid_count > 0:\n",
                "            max_count = max(floor_counts.values())\n",
                "            purity = max_count / valid_count\n",
                "            cluster_purities.append(purity)\n",
                "    \n",
                "    avg_purity = np.mean(cluster_purities) if cluster_purities else 0\n",
                "    \n",
                "    # Küme/Kat oranı - idealden ne kadar sapma var?\n",
                "    cluster_floor_ratio = num_clusters / num_unique_floors if num_unique_floors > 0 else float('inf')\n",
                "    \n",
                "    return {\n",
                "        'num_clusters': num_clusters,\n",
                "        'num_unique_floors': num_unique_floors,\n",
                "        'total_aps': total_aps,\n",
                "        'avg_cluster_size': avg_cluster_size,\n",
                "        'std_cluster_size': std_cluster_size,\n",
                "        'singleton_clusters': singleton_clusters,\n",
                "        'singleton_ratio': singleton_ratio,\n",
                "        'avg_cluster_purity': avg_purity,\n",
                "        'cluster_floor_ratio': cluster_floor_ratio,\n",
                "        'min_cluster_size': min(cluster_sizes) if cluster_sizes else 0,\n",
                "        'max_cluster_size': max(cluster_sizes) if cluster_sizes else 0\n",
                "    }\n",
                "\n",
                "\n",
                "def calculate_adjusted_score(mapped_f1, singleton_ratio, cluster_floor_ratio):\n",
                "    \"\"\"\n",
                "    YENI FONKSİYON: Over-clustering cezası eklenmiş adjusted score\n",
                "    \n",
                "    Args:\n",
                "        mapped_f1: Orijinal F1 skoru\n",
                "        singleton_ratio: Tek elemanlı küme oranı\n",
                "        cluster_floor_ratio: Küme sayısı / Kat sayısı oranı\n",
                "    \n",
                "    Returns:\n",
                "        float: Cezalandırılmış F1 skoru\n",
                "    \"\"\"\n",
                "    # Singleton cezası: %50'den fazlası singleton ise ağır ceza\n",
                "    if singleton_ratio > 0.5:\n",
                "        singleton_penalty = 0.5 * (1 - singleton_ratio)\n",
                "    else:\n",
                "        singleton_penalty = 1.0\n",
                "    \n",
                "    # Over-clustering cezası: İdeal oran 1.0-2.0 arası\n",
                "    if cluster_floor_ratio > 5.0:\n",
                "        overclustering_penalty = max(0.1, 1.0 / (cluster_floor_ratio / 5.0))\n",
                "    elif cluster_floor_ratio > 2.0:\n",
                "        overclustering_penalty = max(0.5, 1.0 / (cluster_floor_ratio / 2.0))\n",
                "    else:\n",
                "        overclustering_penalty = 1.0\n",
                "    \n",
                "    # Final skor\n",
                "    adjusted_score = mapped_f1 * singleton_penalty * overclustering_penalty\n",
                "    \n",
                "    return adjusted_score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def bootstrap_metric(y_true, y_pred, metric_func, n_bootstrap=1000, confidence_level=0.95):\n",
                "    \"\"\"Calculate bootstrap confidence intervals for a metric.\"\"\"\n",
                "    n_samples = len(y_true)\n",
                "    bootstrap_scores = []\n",
                "    \n",
                "    for _ in range(n_bootstrap):\n",
                "        # Bootstrap sampling with replacement\n",
                "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
                "        y_true_boot = y_true[indices]\n",
                "        y_pred_boot = y_pred[indices]\n",
                "        \n",
                "        try:\n",
                "            score = metric_func(y_true_boot, y_pred_boot)\n",
                "            bootstrap_scores.append(score)\n",
                "        except:\n",
                "            continue\n",
                "    \n",
                "    bootstrap_scores = np.array(bootstrap_scores)\n",
                "    alpha = 1 - confidence_level\n",
                "    lower_percentile = (alpha/2) * 100\n",
                "    upper_percentile = (100 - alpha/2)\n",
                "    \n",
                "    ci_lower = np.percentile(bootstrap_scores, lower_percentile)\n",
                "    ci_upper = np.percentile(bootstrap_scores, upper_percentile)\n",
                "    mean_score = np.mean(bootstrap_scores)\n",
                "    std_score = np.std(bootstrap_scores)\n",
                "    \n",
                "    return mean_score, std_score, ci_lower, ci_upper\n",
                "\n",
                "def calculate_metrics_with_ci(y_true, y_pred, n_bootstrap=1000, confidence_level=0.95):\n",
                "    \"\"\"Calculate performance metrics with confidence intervals.\"\"\"\n",
                "    metrics = {}\n",
                "    \n",
                "    # Define metric functions\n",
                "    metric_functions = {\n",
                "        'mapped_accuracy': lambda yt, yp: accuracy_score(yt, yp),\n",
                "        'mapped_precision': lambda yt, yp: precision_score(yt, yp, average='weighted', zero_division=0),\n",
                "        'mapped_recall': lambda yt, yp: recall_score(yt, yp, average='weighted', zero_division=0),\n",
                "        'mapped_f1': lambda yt, yp: f1_score(yt, yp, average='weighted', zero_division=0),\n",
                "    }\n",
                "    \n",
                "    for metric_name, metric_func in metric_functions.items():\n",
                "        mean_score, std_score, ci_lower, ci_upper = bootstrap_metric(\n",
                "            y_true, y_pred, metric_func, n_bootstrap, confidence_level\n",
                "        )\n",
                "        metrics[metric_name] = {\n",
                "            'mean': mean_score,\n",
                "            'std': std_score,\n",
                "            'ci_lower': ci_lower,\n",
                "            'ci_upper': ci_upper\n",
                "        }\n",
                "    \n",
                "    return metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_clustering(y_true, y_pred, prefix=\"\"):\n",
                "    \"\"\"Evaluates clustering performance and returns metrics.\"\"\"\n",
                "    results = {}\n",
                "    results[f\"{prefix}ari\"] = adjusted_rand_score(y_true, y_pred)\n",
                "    results[f\"{prefix}nmi\"] = normalized_mutual_info_score(y_true, y_pred)\n",
                "    results[f\"{prefix}homogeneity\"] = homogeneity_score(y_true, y_pred)\n",
                "    results[f\"{prefix}completeness\"] = completeness_score(y_true, y_pred)\n",
                "    results[f\"{prefix}v_measure\"] = v_measure_score(y_true, y_pred)\n",
                "    if prefix == \"mapped_\":\n",
                "        results[f\"{prefix}accuracy\"] = accuracy_score(y_true, y_pred)\n",
                "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
                "            y_true, y_pred, average='weighted', zero_division=0\n",
                "        )\n",
                "        results[f\"{prefix}precision\"] = precision\n",
                "        results[f\"{prefix}recall\"] = recall\n",
                "        results[f\"{prefix}f1\"] = f1\n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def mcnemar_test(y_true, y_pred1, y_pred2):\n",
                "    \"\"\"Manual implementation of McNemar's test.\"\"\"\n",
                "    # Create contingency table for McNemar's test\n",
                "    correct1 = (y_true == y_pred1).astype(int)\n",
                "    correct2 = (y_true == y_pred2).astype(int)\n",
                "    \n",
                "    # Count discordant pairs\n",
                "    only_1_correct = np.sum((correct1 == 1) & (correct2 == 0))\n",
                "    only_2_correct = np.sum((correct1 == 0) & (correct2 == 1))\n",
                "    \n",
                "    # McNemar's test statistic with continuity correction\n",
                "    if only_1_correct + only_2_correct == 0:\n",
                "        chi2_stat = 0\n",
                "        p_value = 1.0\n",
                "    else:\n",
                "        chi2_stat = (abs(only_1_correct - only_2_correct) - 1)**2 / (only_1_correct + only_2_correct)\n",
                "        p_value = 1 - stats.chi2.cdf(chi2_stat, 1)\n",
                "    \n",
                "    return chi2_stat, p_value, only_1_correct, only_2_correct\n",
                "\n",
                "def statistical_significance_test(y_true, y_pred1, y_pred2, test_type='mcnemar'):\n",
                "    \"\"\"Perform statistical significance test between two predictions.\"\"\"\n",
                "    results = {}\n",
                "    \n",
                "    if test_type == 'mcnemar':\n",
                "        # McNemar's test for paired predictions\n",
                "        chi2_stat, p_value, only_1_correct, only_2_correct = mcnemar_test(y_true, y_pred1, y_pred2)\n",
                "        \n",
                "        results['test_type'] = 'McNemar'\n",
                "        results['chi2_statistic'] = chi2_stat\n",
                "        results['p_value'] = p_value\n",
                "        results['discordant_pairs'] = {\n",
                "            'only_method1_correct': only_1_correct,\n",
                "            'only_method2_correct': only_2_correct\n",
                "        }\n",
                "        \n",
                "    elif test_type == 'paired_t':\n",
                "        # Paired t-test on accuracy differences\n",
                "        acc1 = (y_true == y_pred1).astype(float)\n",
                "        acc2 = (y_true == y_pred2).astype(float)\n",
                "        \n",
                "        t_stat, p_value = stats.ttest_rel(acc1, acc2)\n",
                "        \n",
                "        results['test_type'] = 'Paired t-test'\n",
                "        results['t_statistic'] = t_stat\n",
                "        results['p_value'] = p_value\n",
                "        results['mean_difference'] = np.mean(acc1 - acc2)\n",
                "        \n",
                "    elif test_type == 'wilcoxon':\n",
                "        # Wilcoxon signed-rank test (non-parametric alternative)\n",
                "        acc1 = (y_true == y_pred1).astype(float)\n",
                "        acc2 = (y_true == y_pred2).astype(float)\n",
                "        \n",
                "        try:\n",
                "            stat, p_value = stats.wilcoxon(acc1, acc2, alternative='two-sided')\n",
                "            results['test_type'] = 'Wilcoxon Signed-Rank'\n",
                "            results['statistic'] = stat\n",
                "            results['p_value'] = p_value\n",
                "        except ValueError:\n",
                "            # All differences are zero\n",
                "            results['test_type'] = 'Wilcoxon Signed-Rank'\n",
                "            results['statistic'] = 0\n",
                "            results['p_value'] = 1.0\n",
                "            \n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_confusion_matrix_with_ci(y_true, y_pred, output_file, title, n_bootstrap=1000):\n",
                "    \"\"\"Generates confusion matrix with confidence intervals for each cell.\"\"\"\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    n_samples = len(y_true)\n",
                "    \n",
                "    # Bootstrap confidence intervals for confusion matrix cells\n",
                "    cm_bootstrap = []\n",
                "    for _ in range(n_bootstrap):\n",
                "        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
                "        y_true_boot = y_true[indices]\n",
                "        y_pred_boot = y_pred[indices]\n",
                "        cm_boot = confusion_matrix(y_true_boot, y_pred_boot, labels=np.unique(y_true))\n",
                "        cm_bootstrap.append(cm_boot)\n",
                "    \n",
                "    cm_bootstrap = np.array(cm_bootstrap)\n",
                "    cm_std = np.std(cm_bootstrap, axis=0)\n",
                "    \n",
                "    # Set up the plot\n",
                "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
                "    \n",
                "    plt.rcParams.update({\n",
                "        'font.size': 12,\n",
                "        'axes.titlesize': 14,\n",
                "        'axes.labelsize': 12,\n",
                "    })\n",
                "    \n",
                "    # Original confusion matrix\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
                "                annot_kws={'size': 10}, cbar_kws={'shrink': 0.8})\n",
                "    ax1.set_xlabel('Predicted')\n",
                "    ax1.set_ylabel('True')\n",
                "    ax1.set_title(f'{title}\\n(Counts)')\n",
                "    \n",
                "    # Confusion matrix with standard errors\n",
                "    annotations = []\n",
                "    for i in range(cm.shape[0]):\n",
                "        row = []\n",
                "        for j in range(cm.shape[1]):\n",
                "            row.append(f'{cm[i,j]}\\n±{cm_std[i,j]:.1f}')\n",
                "        annotations.append(row)\n",
                "    \n",
                "    sns.heatmap(cm_std, annot=annotations, fmt='', cmap='Reds', ax=ax2,\n",
                "                annot_kws={'size': 8}, cbar_kws={'shrink': 0.8})\n",
                "    ax2.set_xlabel('Predicted')\n",
                "    ax2.set_ylabel('True')\n",
                "    ax2.set_title(f'{title}\\n(Counts ± Bootstrap SE)')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(output_file, dpi=300, bbox_inches='tight')\n",
                "    plt.show()\n",
                "    plt.close()\n",
                "    plt.rcParams.update(plt.rcParamsDefault)\n",
                "\n",
                "\n",
                "def analyze_clusters(clusters, ap_to_floor, output_dir):\n",
                "    \"\"\"Analyzes each cluster and visualizes floor distribution.\"\"\"\n",
                "    results = []\n",
                "    for cluster_id, aps in enumerate(clusters):\n",
                "        floor_counts = defaultdict(int)\n",
                "        valid_aps = 0\n",
                "        for ap in aps:\n",
                "            if ap in ap_to_floor:\n",
                "                floor_counts[ap_to_floor[ap]] += 1\n",
                "                valid_aps += 1\n",
                "        if not valid_aps:\n",
                "            continue\n",
                "        percentages = {floor: (count / valid_aps) * 100 for floor, count in floor_counts.items()}\n",
                "        dominant_floor = max(percentages.items(), key=lambda x: x[1]) if percentages else (None, 0)\n",
                "        results.append({\n",
                "            'cluster_id': cluster_id,\n",
                "            'total_aps': len(aps),\n",
                "            'valid_aps': valid_aps,\n",
                "            'floor_counts': dict(floor_counts),\n",
                "            'floor_percentages': percentages,\n",
                "            'dominant_floor': dominant_floor[0],\n",
                "            'dominant_percentage': dominant_floor[1]\n",
                "        })\n",
                "    \n",
                "    with open(os.path.join(output_dir, 'cluster_analysis.json'), 'w', encoding='utf-8') as f:\n",
                "        json.dump(results, f, ensure_ascii=False, indent=4)\n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_the_way(path, file_format, con=\"\"):\n",
                "    files_add = []\n",
                "    for r, d, f in os.walk(path):\n",
                "        for file in f:\n",
                "            if file_format in file:\n",
                "                if con in file:\n",
                "                    files_add.append(os.path.join(r, file))\n",
                "    return files_add\n",
                "\n",
                "def compile_metrics_to_table(root_dir):\n",
                "    \"\"\"\n",
                "    Belirtilen dizin ve alt dizinlerindeki tüm performance_metrics.json\n",
                "    dosyalarını okur ve bir Pandas DataFrame (tablo) haline getirir.\n",
                "    \"\"\"\n",
                "    all_data = []\n",
                "\n",
                "    # os.walk ile tüm alt klasörleri geziyoruz\n",
                "    for root, dirs, files in os.walk(root_dir):\n",
                "        if 'performance_metrics.json' in files:\n",
                "            file_path = os.path.join(root, 'performance_metrics.json')\n",
                "            \n",
                "            # Klasör adını al (Örn: Node2Vec_communities)\n",
                "            folder_name = os.path.basename(root)\n",
                "            \n",
                "            try:\n",
                "                with open(file_path, 'r', encoding='utf-8') as f:\n",
                "                    metrics = json.load(f)\n",
                "                    \n",
                "                    # Düzleştirme (Flattening) logic for nested dicts like 'metrics_ci' or 'baseline_comparison'\n",
                "                    flat_metrics = {}\n",
                "                    flat_metrics['Algorithm'] = folder_name\n",
                "                    \n",
                "                    # Basic keys\n",
                "                    for k, v in metrics.items():\n",
                "                        if not isinstance(v, dict):\n",
                "                            flat_metrics[k] = v\n",
                "                    \n",
                "                    # Nested keys (Metrics with CI)\n",
                "                    if 'metrics_ci' in metrics:\n",
                "                        for m_name, m_vals in metrics['metrics_ci'].items():\n",
                "                            flat_metrics[f\"{m_name}_mean\"] = m_vals['mean']\n",
                "                            flat_metrics[f\"{m_name}_ci_lower\"] = m_vals['ci_lower']\n",
                "                            flat_metrics[f\"{m_name}_ci_upper\"] = m_vals['ci_upper']\n",
                "\n",
                "                    # Nested keys (Statistical comparison)\n",
                "                    if 'baseline_comparison' in metrics:\n",
                "                        for test_name, test_vals in metrics['baseline_comparison'].items():\n",
                "                            flat_metrics[f\"p_value_{test_name}\"] = test_vals['p_value']\n",
                "\n",
                "                    all_data.append(flat_metrics)\n",
                "            except Exception as e:\n",
                "                print(f\"Hata: {file_path} dosyası okunamadı. Sebebi: {e}\")\n",
                "\n",
                "    # Listeyi DataFrame'e (Tabloya) çevir\n",
                "    if all_data:\n",
                "        df = pd.DataFrame(all_data)\n",
                "        \n",
                "        # 'Algorithm' sütununu en başa al\n",
                "        cols = ['Algorithm'] + [c for c in df.columns if c != 'Algorithm']\n",
                "        df = df[cols]\n",
                "        \n",
                "        # Algoritma ismine göre sırala (isteğe bağlı)\n",
                "        df = df.sort_values(by='Algorithm').reset_index(drop=True)\n",
                "        \n",
                "        return df\n",
                "    else:\n",
                "        print(\"Hiçbir performance_metrics.json dosyası bulunamadı.\")\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main Execution\n",
                "paths = [args.path]\n",
                "\n",
                "# Load baseline if provided\n",
                "baseline_ap_to_cluster = None\n",
                "baseline_clusters = None\n",
                "if args.baseline_file:\n",
                "    print(f\"Loading baseline from: {args.baseline_file}\")\n",
                "    try:\n",
                "        baseline_ap_to_cluster, baseline_clusters = load_clustering_result(args.baseline_file)\n",
                "    except Exception as e:\n",
                "        print(f\"Error loading baseline file: {e}\")\n",
                "\n",
                "# Iterate paths\n",
                "for p in paths:\n",
                "    path = f\"{p}/community_results\"\n",
                "    if not os.path.exists(path):\n",
                "        print(f\"Directory not found: {path}\")\n",
                "        continue\n",
                "        \n",
                "    files_add = find_the_way(path, '_communities.csv')\n",
                "    print(f\"Found {len(files_add)} result files.\")\n",
                "\n",
                "    for file in files_add:\n",
                "        print(f\"\\nProcessing: {file}\")\n",
                "        # 1. Setup file paths\n",
                "        gt_file = os.path.join(p, 'data_GT.json')\n",
                "        result_file = file\n",
                "        \n",
                "        if not os.path.exists(gt_file):\n",
                "            print(f\"Ground truth file not found: {gt_file}\")\n",
                "            continue\n",
                "        \n",
                "        folder_name = os.path.basename(file).replace('_communities.csv', '') + '_results'\n",
                "        \n",
                "        output_dir = os.path.join(p, 'results', folder_name)\n",
                "        os.makedirs(output_dir, exist_ok=True)\n",
                "        \n",
                "        # 2. Load data\n",
                "        ap_to_floor = load_ground_truth(gt_file)\n",
                "        ap_to_cluster, clusters = load_clustering_result(result_file)\n",
                "        cluster_to_floor = map_clusters_to_floors(ap_to_floor, ap_to_cluster, clusters)\n",
                "        common_aps, y_true, y_pred_raw, y_pred_mapped = create_true_pred_arrays(\n",
                "            ap_to_floor, ap_to_cluster, cluster_to_floor\n",
                "        )\n",
                "        \n",
                "        # 3. Evaluate clustering\n",
                "        raw_results = evaluate_clustering(y_true, y_pred_raw, prefix=\"raw_\")\n",
                "        mapped_results = evaluate_clustering(y_true, y_pred_mapped, prefix=\"mapped_\")\n",
                "        \n",
                "        # 3.5 Calculate Metrics with CI\n",
                "        print(\"Calculating metrics with Bootstrap CI...\")\n",
                "        metrics_with_ci = calculate_metrics_with_ci(\n",
                "            y_true, y_pred_mapped, \n",
                "            n_bootstrap=args.n_bootstrap, \n",
                "            confidence_level=args.confidence_level\n",
                "        )\n",
                "        \n",
                "        # 4. *** YENI: Küme kalite metrikleri ***\n",
                "        quality_metrics = calculate_cluster_quality_metrics(clusters, ap_to_floor)\n",
                "        \n",
                "        # 5. *** YENI: Adjusted score hesapla ***\n",
                "        adjusted_f1 = calculate_adjusted_score(\n",
                "            mapped_results['mapped_f1'],\n",
                "            quality_metrics['singleton_ratio'],\n",
                "            quality_metrics['cluster_floor_ratio']\n",
                "        )\n",
                "        \n",
                "        # Statistical Comparison with Baseline\n",
                "        statistical_results = {}\n",
                "        if baseline_ap_to_cluster:\n",
                "            print(\"Performing statistical comparison with baseline...\")\n",
                "            baseline_cluster_to_floor = map_clusters_to_floors(ap_to_floor, baseline_ap_to_cluster, baseline_clusters)\n",
                "            _, _, _, y_pred_baseline = create_true_pred_arrays(\n",
                "                ap_to_floor, baseline_ap_to_cluster, baseline_cluster_to_floor\n",
                "            )\n",
                "            \n",
                "            common_all = sorted(set(ap_to_floor.keys()) & set(ap_to_cluster.keys()) & set(baseline_ap_to_cluster.keys()))\n",
                "            y_true_common = np.array([ap_to_floor[ap] for ap in common_all])\n",
                "            y_pred_mapped_common = np.array([cluster_to_floor[ap_to_cluster[ap]] for ap in common_all])\n",
                "            y_pred_baseline_common = np.array([baseline_cluster_to_floor[baseline_ap_to_cluster[ap]] for ap in common_all])\n",
                "            \n",
                "            sig_test_mcnemar = statistical_significance_test(y_true_common, y_pred_mapped_common, y_pred_baseline_common, 'mcnemar')\n",
                "            sig_test_ttest = statistical_significance_test(y_true_common, y_pred_mapped_common, y_pred_baseline_common, 'paired_t')\n",
                "            sig_test_wilcoxon = statistical_significance_test(y_true_common, y_pred_mapped_common, y_pred_baseline_common, 'wilcoxon')\n",
                "            \n",
                "            statistical_results = {\n",
                "                'baseline_comparison': {\n",
                "                    'mcnemar': sig_test_mcnemar,\n",
                "                    'paired_t': sig_test_ttest,\n",
                "                    'wilcoxon': sig_test_wilcoxon\n",
                "                }\n",
                "            }\n",
                "\n",
                "        # Tüm sonuçları birleştir\n",
                "        all_results = {\n",
                "            **raw_results,\n",
                "            **mapped_results,\n",
                "            **quality_metrics,\n",
                "            'adjusted_f1_score': adjusted_f1,\n",
                "            'metrics_ci': metrics_with_ci,\n",
                "            **statistical_results\n",
                "        }\n",
                "        \n",
                "        # 6. Print Results\n",
                "        print(f\"\\n{'='*60}\")\n",
                "        print(f\"ALGORITMA: {folder_name}\")\n",
                "        print(f\"{'='*60}\")\n",
                "        \n",
                "        print(\"\\n--- Kümeleme İstatistikleri ---\")\n",
                "        print(f\"Toplam AP Sayısı: {quality_metrics['total_aps']}\")\n",
                "        print(f\"Küme Sayısı: {quality_metrics['num_clusters']}\")\n",
                "        print(f\"Gerçek Kat Sayısı: {quality_metrics['num_unique_floors']}\")\n",
                "        print(f\"Küme/Kat Oranı: {quality_metrics['cluster_floor_ratio']:.2f}\")\n",
                "        print(f\"Singleton (Tek Elemanlı) Küme Sayısı: {quality_metrics['singleton_clusters']}\")\n",
                "        print(f\"Singleton Oranı: {quality_metrics['singleton_ratio']:.2%}\")\n",
                "        print(f\"Ortalama Küme Boyutu: {quality_metrics['avg_cluster_size']:.2f}\")\n",
                "        print(f\"Ortalama Küme Saflığı: {quality_metrics['avg_cluster_purity']:.2%}\")\n",
                "        \n",
                "        print(\"\\n--- Raw Cluster Evaluation ---\")\n",
                "        print(f\"Adjusted Rand Index: {raw_results['raw_ari']:.4f}\")\n",
                "        print(f\"Normalized Mutual Information: {raw_results['raw_nmi']:.4f}\")\n",
                "        \n",
                "        print(\"\\n--- Mapped Evaluation (with CI) ---\")\n",
                "        print(f\"Accuracy: {mapped_results['mapped_accuracy']:.4f} (95% CI: [{metrics_with_ci['mapped_accuracy']['ci_lower']:.4f}, {metrics_with_ci['mapped_accuracy']['ci_upper']:.4f}])\")\n",
                "        print(f\"F1-Score (Orijinal): {mapped_results['mapped_f1']:.4f} (95% CI: [{metrics_with_ci['mapped_f1']['ci_lower']:.4f}, {metrics_with_ci['mapped_f1']['ci_upper']:.4f}])\")\n",
                "        print(f\"F1-Score (Adjusted): {adjusted_f1:.4f} ⚠️\")\n",
                "        \n",
                "        if statistical_results:\n",
                "            print(\"\\n--- Statistical Significance vs Baseline ---\")\n",
                "            print(f\"McNemar p-value: {statistical_results['baseline_comparison']['mcnemar']['p_value']:.6f}\")\n",
                "            print(f\"Paired t-test p-value: {statistical_results['baseline_comparison']['paired_t']['p_value']:.6f}\")\n",
                "            print(f\"Wilcoxon p-value: {statistical_results['baseline_comparison']['wilcoxon']['p_value']:.6f}\")\n",
                "\n",
                "        # *** YENI: UYARI SİSTEMİ ***\n",
                "        warnings_list = []\n",
                "        if quality_metrics['singleton_ratio'] > 0.3:\n",
                "            warnings_list.append(f\"⚠️  UYARI: Singleton küme oranı çok yüksek ({quality_metrics['singleton_ratio']:.1%})!\")\n",
                "        if quality_metrics['cluster_floor_ratio'] > 3:\n",
                "            warnings_list.append(f\"⚠️  UYARI: Küme sayısı kat sayısının {quality_metrics['cluster_floor_ratio']:.1f}x fazla!\")\n",
                "        if quality_metrics['avg_cluster_size'] < 2:\n",
                "            warnings_list.append(f\"⚠️  UYARI: Ortalama küme boyutu çok küçük ({quality_metrics['avg_cluster_size']:.1f})!\")\n",
                "        \n",
                "        if warnings_list:\n",
                "            print(\"\\n\" + \"=\"*60)\n",
                "            for warning in warnings_list:\n",
                "                print(warning)\n",
                "            print(\"=\"*60)\n",
                "        \n",
                "        # 7. Save results\n",
                "        with open(os.path.join(output_dir, 'performance_metrics.json'), 'w', encoding='utf-8') as f:\n",
                "            json.dump(all_results, f, ensure_ascii=False, indent=4)\n",
                "        \n",
                "        # 8. Generate plots\n",
                "        generate_confusion_matrix_with_ci(\n",
                "            y_true, y_pred_mapped,\n",
                "            os.path.join(output_dir, 'confusion_matrix_mapped_ci.pdf'),\n",
                "            'Confusion Matrix for Mapped Clusters\\n(with Bootstrap CI)',\n",
                "            n_bootstrap=args.n_bootstrap\n",
                "        )\n",
                "        \n",
                "        cluster_analysis = analyze_clusters(clusters, ap_to_floor, output_dir)\n",
                "        \n",
                "        print(f\"\\nTüm sonuçlar kaydedildi: {output_dir}\")\n",
                "\n",
                "# Compile table\n",
                "for p in paths:\n",
                "    search_path = f\"{p}/results\"\n",
                "    df_results = compile_metrics_to_table(search_path)\n",
                "    \n",
                "    if df_results is not None:\n",
                "        print(\"\\nTüm Sonuçlar Tablosu:\")\n",
                "        display(df_results)\n",
                "        df_results.to_excel(os.path.join(p, \"sonuc.xlsx\"), index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}