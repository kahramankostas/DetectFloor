{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1d9f0e-99bc-4912-a4d3-4de548afe535",
   "metadata": {},
   "source": [
    "# zip export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ab0a16-34d1-4a60-87d1-f6b3a13ba6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 inner zip files.\n",
      "[1] Extracting 'datasets/WLAN (WiFi) RSS database for fingerprinting positioning.zip' ‚Üí data\\1\n",
      "   ‚îî‚îÄ Flattened folder structure inside 'data\\1'\n",
      "[2] Extracting 'datasets/WiFi RSS measurements in Tampere University multi-building campus 2017 - Zenodo 5174851.zip' ‚Üí data\\2\n",
      "   ‚îî‚îÄ Flattened folder structure inside 'data\\2'\n",
      "[3] Extracting 'datasets/Wi-Fi Fingerprinting dataset with multiple simultaneous interfaces.zip' ‚Üí data\\3\n",
      "   ‚îî‚îÄ Flattened folder structure inside 'data\\3'\n",
      "[4] Extracting 'datasets/UJIndoorLoc.zip' ‚Üí data\\4\n",
      "   ‚îî‚îÄ Flattened folder structure inside 'data\\4'\n",
      "[5] Extracting 'datasets/DSI_dataset.zip' ‚Üí data\\5\n",
      "[6] Extracting 'datasets/Crowdsourced WiFi database and benchmark software for indoor positioning.zip' ‚Üí data\\6\n",
      "‚úÖ All datasets extracted and flattened successfully.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "DATASET_ORDER = [\n",
    "    \"WLAN (WiFi) RSS database for fingerprinting positioning\",\n",
    "    \"WiFi RSS measurements in Tampere University multi-building campus 2017\",\n",
    "    \"Wi-Fi Fingerprinting dataset with multiple simultaneous interfaces\",\n",
    "    \"UJIndoorLoc\",\n",
    "    \"DSI_dataset\",\n",
    "    \"Crowdsourced WiFi database and benchmark software for indoor positioning\"\n",
    "]\n",
    "\n",
    "def flatten_folder(folder_path):\n",
    "    \"\"\"Eƒüer klas√∂rde sadece tek bir alt klas√∂r varsa, onun i√ßeriƒüini yukarƒ± ta≈üƒ±r.\"\"\"\n",
    "    entries = os.listdir(folder_path)\n",
    "    if len(entries) == 1:\n",
    "        subfolder = os.path.join(folder_path, entries[0])\n",
    "        if os.path.isdir(subfolder):\n",
    "            for item in os.listdir(subfolder):\n",
    "                src = os.path.join(subfolder, item)\n",
    "                dst = os.path.join(folder_path, item)\n",
    "                shutil.move(src, dst)\n",
    "            shutil.rmtree(subfolder)\n",
    "            print(f\"   ‚îî‚îÄ Flattened folder structure inside '{folder_path}'\")\n",
    "\n",
    "def extract_datasets(outer_zip=\"datasets.zip\", output_dir=\"data\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with zipfile.ZipFile(outer_zip, 'r') as outer:\n",
    "        inner_files = [f for f in outer.namelist() if f.lower().endswith(\".zip\")]\n",
    "        print(f\"Found {len(inner_files)} inner zip files.\")\n",
    "        \n",
    "        for idx, pattern in enumerate(DATASET_ORDER, start=1):\n",
    "            match = next((f for f in inner_files if pattern.lower() in f.lower()), None)\n",
    "            if not match:\n",
    "                print(f\"‚ö†Ô∏è Could not find dataset for pattern: {pattern}\")\n",
    "                continue\n",
    "            \n",
    "            target_folder = os.path.join(output_dir, str(idx))\n",
    "            os.makedirs(target_folder, exist_ok=True)\n",
    "            \n",
    "            print(f\"[{idx}] Extracting '{match}' ‚Üí {target_folder}\")\n",
    "            with outer.open(match) as inner_zip_file:\n",
    "                with zipfile.ZipFile(inner_zip_file) as inner_zip:\n",
    "                    inner_zip.extractall(target_folder)\n",
    "            \n",
    "            # Flatten klas√∂r yapƒ±sƒ±\n",
    "            flatten_folder(target_folder)\n",
    "    \n",
    "    print(\"‚úÖ All datasets extracted and flattened successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extract_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e177c00d-d81b-4c1c-a851-3e51f3cd835d",
   "metadata": {},
   "source": [
    "#### Converts RSS and coordinate  information from different WiFi positioning (fingerprinting) data sets to JSON format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53098376-74f8-40a2-a1f7-3eb035979107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ  ./data/4/trainingData.csv\n",
      "‚úÖ  ./data/4/validationData.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "def find_the_way(path,file_format,con=\"\"):\n",
    "    files_add = []\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if file_format in file:\n",
    "                if con in file:\n",
    "                    files_add.append(os.path.join(r, file))  \n",
    "            \n",
    "    return files_add\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "files_add=find_the_way(\"./data/4/\",\".csv\")\n",
    "files_add\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for file in files_add :\n",
    "    j_rssi = defaultdict(dict)\n",
    "    j_coo = defaultdict(dict)\n",
    "    rssis= pd.read_csv(file)#,header=None)\n",
    "    cols=['LONGITUDE', 'LATITUDE', 'FLOOR', 'BUILDINGID', 'SPACEID','RELATIVEPOSITION', 'USERID', 'PHONEID', 'TIMESTAMP']\n",
    "    lon=rssis['LONGITUDE'].values\n",
    "    lat=rssis['LATITUDE'].values\n",
    "    for col in cols:\n",
    "        del rssis[col]\n",
    "    for ii,i in enumerate(rssis.values):\n",
    "        for jj, j in enumerate(i):\n",
    "            if j!=100:\n",
    "                j_rssi[int(ii)][int(jj)]=int(j)\n",
    "                j_coo[ii]=[lon[ii],lat[ii]]\n",
    "   \n",
    "    with open(file.replace(\"csv\",\"_rss.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(j_rssi, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    with open(file.replace(\"csv\",\"_coo.json\"), 'w', encoding='utf-8') as f:\n",
    "        json.dump(j_coo, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"‚úÖ  {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748f900-083a-45af-ae2c-ca238e2124a4",
   "metadata": {},
   "source": [
    "#### Calculates the distances between different measurement points in WiFi positioning data (based on coordinates) and saves them to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebab5be1-5ee3-4349-97d1-931a9fec9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import math\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8dd8abf-4511-4813-a219-d3cac08723ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_global_coordinate(coord):\n",
    "    \"\"\"\n",
    "   Checks whether the coordinates are global.\n",
    "   - If the latitude is between (-90, 90) and the longitude is between (-180, 180), they are global.\n",
    "   - If both coordinates are above 90, they are considered local X/Y.\n",
    "    \"\"\"\n",
    "    lat, lon = coord\n",
    "    if -90 <= lat <= 90 and -180 <= lon <= 180:\n",
    "        return True  # K√ºresel koordinat (lat/lon)\n",
    "    if lat > 90 and lon > 90:\n",
    "        return False  # Yerel koordinat (X/Y)\n",
    "    \n",
    "    # ≈û√ºpheli durumlarda uyarƒ± ver\n",
    "    print(f\"UYARI: Koordinatlar tam tespit edilemedi, varsayƒ±lan olarak yerel kabul ediliyor! {coord}\")\n",
    "    return False  # Varsayƒ±lan olarak yerel kabul et\n",
    "\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Haversine form√ºl√º ile iki k√ºresel koordinat arasƒ±ndaki mesafeyi hesaplar (km cinsinden).\"\"\"\n",
    "    R = 6371  # D√ºnya'nƒ±n yarƒ±√ßapƒ± (km)\n",
    "    \n",
    "    # Dereceden radyana √ßevirme\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Farklar\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Haversine form√ºl√º\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    return R * c  # Sonu√ß km cinsinden d√∂nd√ºr√ºl√ºr\n",
    "\n",
    "def euclidean_distance(x1, y1, x2, y2):\n",
    "    \"\"\"√ñklidyen mesafe hesaplar (yerel koordinatlar i√ßin).\"\"\"\n",
    "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "def calculate_distance(coord1, coord2):\n",
    "    \"\"\"\n",
    "    Koordinatlarƒ±n t√ºr√ºne g√∂re uygun mesafe hesaplama y√∂ntemi se√ßer.\n",
    "    - Eƒüer her iki koordinat da lat/lon ise Haversine\n",
    "    - Eƒüer her ikisi de x/y ise √ñklidyen\n",
    "    - Karƒ±≈üƒ±k giri≈ülerde None d√∂nd√ºrerek i≈ülemi atlar.\n",
    "    \"\"\"\n",
    "    is_global_1 = is_global_coordinate(coord1)\n",
    "    is_global_2 = is_global_coordinate(coord2)\n",
    "\n",
    "    if is_global_1 and is_global_2:\n",
    "        return haversine_distance(*coord1, *coord2), \"Haversine\"\n",
    "    elif not is_global_1 and not is_global_2:\n",
    "        return euclidean_distance(*coord1, *coord2), \"Euclidean\"\n",
    "    else:\n",
    "        return None, \"Ge√ßersiz\"  # None d√∂nd√ºrerek hatayƒ± √∂nle\n",
    "\n",
    "\n",
    "def intersection_count(list1, list2):\n",
    "    \"\"\"ƒ∞ki listenin ortak eleman sayƒ±sƒ±nƒ± d√∂nd√ºr√ºr.\"\"\"\n",
    "    return len(set(list1) & set(list2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a9340d6-cfa2-4e4d-941f-edfcebb4f010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/4\\trainingData._rss.json\n",
      "./data/4\\validationData._rss.json\n"
     ]
    }
   ],
   "source": [
    "prelist=find_the_way(\"./data/4\",\".json\",\"\")\n",
    "files_add =  dict(zip(prelist[1::2], prelist[0::2]))\n",
    "for i in files_add:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c0a0177-bd0a-4d34-a051-7559e01ab70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197219730/197219730 [21:47<00:00, 150872.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/4\\trainingData._rss.json  kaydedildi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 616605/616605 [00:04<00:00, 125229.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/4\\validationData._rss.json  kaydedildi.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for file in files_add:\n",
    "    # JSON dosyalarƒ±nƒ± oku\n",
    "    with open(file, \"r\") as f:\n",
    "         rss= json.load(f)\n",
    "\n",
    "\n",
    "    with open(files_add[file], \"r\") as f:\n",
    "        data  = json.load(f)\n",
    "\n",
    "        \n",
    "    # Verileri uygun formata √ßevir (Anahtar: Nokta ID, Deƒüer: (x, y) veya (lat, lon))\n",
    "    points = {int(k): tuple(v) for k, v in data.items()}\n",
    "    # CSV dosyasƒ±nƒ± a√ß ve verileri anlƒ±k olarak yaz\n",
    "    with open(file.replace(\".json\",\"@.csv\"), \"w\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"id1\", \"id2\", \"estimated_distance\", \"method\"])  # Ba≈ülƒ±k satƒ±rƒ±\n",
    "\t\t\n",
    "\n",
    "\n",
    "        total = len(points) * (len(points) - 1) // 2  # kombinasyon sayƒ±sƒ±\n",
    "\n",
    "        for (i, point1), (j, point2) in tqdm(itertools.combinations(points.items(), 2), total=total):\n",
    "\n",
    "                list1 = list(rss[str(i)].keys())\n",
    "                list2 = list(rss[str(j)].keys())\n",
    "        \n",
    "                if intersection_count(list1, list2):  # Ortak veri varsa mesafeyi hesapla\n",
    "                    try:\n",
    "                        distance = euclidean_distance(point1[0],point1[1],point2[0],point2[1])\n",
    "                        writer.writerow([i, j, round(distance, 2), \"euclidean_distance\"])  # Dosyaya yaz\n",
    "                    except ValueError as e:\n",
    "                        print(f\"Hata: {e}\")\n",
    "    print(f\"{file}  kaydedildi.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33bf6d4-b138-4377-a6aa-caf4816f6c4e",
   "metadata": {},
   "source": [
    "####  It calculates various distance metrics (e.g. correlation, Euclidean, cosine, Jaccard) to measure the similarity or difference between WiFi fingerprint data, and records these as feature extraction in CSV files.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4414b594-6ab0-4240-929d-f900210f0338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import scipy.spatial.distance\n",
    "from tqdm import tqdm\n",
    "\n",
    "## 1.4 Feature Extraction (Intersection Based) - Simplified\n",
    "def feature_extraction_file(data, name, fps):\n",
    "    features = [[\"correlation\",\n",
    "                \"chebyshev\", \n",
    "                \"intersecting_mac\",\n",
    "                \"euclidean\",\n",
    "                \"cosine\",\n",
    "                \"jensenshannon\",\n",
    "                \"jaccard\",\n",
    "                \"canberra\",\n",
    "                \"minkowski\",\n",
    "                \"real\"]]\n",
    "    \n",
    "    for i in tqdm(data, position=0, leave=True):\n",
    "        fp1 = fps[i[0]]\n",
    "        fp2 = fps[i[1]]\n",
    "        feature = feature_extraction(fp1, fp2) \n",
    "        feature.append(i[2])\n",
    "        features.append(feature)\n",
    "    \n",
    "    with open(name, \"w\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(features) \n",
    "\n",
    "def feature_extraction(fp1, fp2):\n",
    "    # Kesi≈üim (intersection) kullanarak ortak MAC adreslerini bul\n",
    "    common_macs = set(fp1.keys()).intersection(set(fp2.keys()))\n",
    "    \n",
    "    # Eƒüer kesi≈üim bo≈üsa, t√ºm mesafeleri 0 olarak d√∂nd√ºr\n",
    "    if not common_macs:\n",
    "        intersecting_mac = 0\n",
    "        # Bo≈ü vekt√∂rler i√ßin mesafe hesaplamalarƒ±\n",
    "        output_data = [0, 0, intersecting_mac, 0, 0, 0, 0, 0, 0]\n",
    "        return output_data\n",
    "    \n",
    "    # Kesi≈üen MAC sayƒ±sƒ±\n",
    "    intersecting_mac = len(common_macs)\n",
    "    \n",
    "    # Sadece ortak MAC'ler i√ßin vekt√∂rleri olu≈ütur\n",
    "    f1 = [fp1[mac] for mac in common_macs]\n",
    "    f2 = [fp2[mac] for mac in common_macs]\n",
    "    \n",
    "    # G√ºvenli mesafe hesaplamalarƒ±\n",
    "    def safe_distance(func, v1, v2, default_value=0):\n",
    "        \"\"\"Mesafe hesaplama fonksiyonunu g√ºvenli ≈üekilde √ßalƒ±≈ütƒ±rƒ±r\"\"\"\n",
    "        try:\n",
    "            result = func(v1, v2)\n",
    "            if np.isnan(result) or np.isinf(result):\n",
    "                return default_value\n",
    "            return result\n",
    "        except (ValueError, ZeroDivisionError, RuntimeWarning):\n",
    "            return default_value\n",
    "    \n",
    "    # Vekt√∂rleri numpy array'e √ßevir\n",
    "    f1_arr = np.array(f1, dtype=float)\n",
    "    f2_arr = np.array(f2, dtype=float)\n",
    "    \n",
    "    # √ñzel durumlarƒ± kontrol et\n",
    "    f1_std = np.std(f1_arr)\n",
    "    f2_std = np.std(f2_arr)\n",
    "    \n",
    "    # Mesafe hesaplamalarƒ±\n",
    "    correlation = safe_distance(scipy.spatial.distance.correlation, f1_arr, f2_arr, 1.0)\n",
    "    chebyshev = safe_distance(scipy.spatial.distance.chebyshev, f1_arr, f2_arr, 0)\n",
    "    euclidean = safe_distance(scipy.spatial.distance.euclidean, f1_arr, f2_arr, 0)\n",
    "    \n",
    "    # Cosine i√ßin √∂zel kontrol (sƒ±fƒ±r vekt√∂r kontrol√º)\n",
    "    if f1_std == 0 and f2_std == 0:\n",
    "        cosine = 0  # ƒ∞ki vekt√∂r de sabit ise mesafe sƒ±fƒ±r\n",
    "    elif f1_std == 0 or f2_std == 0:\n",
    "        cosine = 1  # Bir vekt√∂r sabit ise maksimum mesafe\n",
    "    else:\n",
    "        cosine = safe_distance(scipy.spatial.distance.cosine, f1_arr, f2_arr, 1.0)\n",
    "    \n",
    "    jensenshannon = safe_distance(scipy.spatial.distance.jensenshannon, f1_arr, f2_arr, 0)\n",
    "    canberra = safe_distance(scipy.spatial.distance.canberra, f1_arr, f2_arr, 0)\n",
    "    minkowski = safe_distance(scipy.spatial.distance.minkowski, f1_arr, f2_arr, 0)\n",
    "    \n",
    "    # Jaccard mesafesi i√ßin binary vekt√∂rler (g√ºvenli hesaplama)\n",
    "    threshold = -70  \n",
    "    f1_binary = [1 if x > threshold else 0 for x in f1]\n",
    "    f2_binary = [1 if x > threshold else 0 for x in f2]\n",
    "    \n",
    "    # Jaccard i√ßin √∂zel kontrol (t√ºm sƒ±fƒ±r vekt√∂r kontrol√º)\n",
    "    if sum(f1_binary) == 0 and sum(f2_binary) == 0:\n",
    "        jaccard = 0  # ƒ∞ki vekt√∂r de t√ºm sƒ±fƒ±r ise benzer\n",
    "    elif sum(f1_binary) == 0 or sum(f2_binary) == 0:\n",
    "        jaccard = 1  # Bir vekt√∂r t√ºm sƒ±fƒ±r ise maksimum mesafe\n",
    "    else:\n",
    "        jaccard = safe_distance(scipy.spatial.distance.jaccard, f1_binary, f2_binary, 1.0)\n",
    "    \n",
    "    output_data = [correlation,\n",
    "                  chebyshev,\n",
    "                  intersecting_mac,\n",
    "                  euclidean,\n",
    "                  cosine,\n",
    "                  jensenshannon,\n",
    "                  jaccard,\n",
    "                  canberra,\n",
    "                  minkowski]\n",
    "    \n",
    "    # NaN deƒüerleri 0 ile deƒüi≈ütir\n",
    "    output_data = [0 if x != x else x for x in output_data]\n",
    "    \n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82555e80-8f7e-4ffe-a4b0-76945d8a2fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/4/validationData._rss@.csv']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prelist=find_the_way(\"./data/4/\",\"@.csv\",\"\")\n",
    "prelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17cb2286-b32a-489f-a0e3-d00c563e6930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/4/validationData._rss@.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174416it [00:00, 179518.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data loaded!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 0/174416 [00:00<?, ?it/s]C:\\Users\\kahra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\spatial\\distance.py:647: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / math.sqrt(uu * vv)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 174416/174416 [01:10<00:00, 2479.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "for p in prelist:\n",
    "\n",
    "    print(p)\n",
    "\n",
    "    with open(p.replace(\"@.csv\",\".json\")) as f:\n",
    "        fps_train = json.load(f)\n",
    "    \n",
    "    with open(p) as f:\n",
    "        train_data = []\n",
    "        train_h = csv.DictReader(f)\n",
    "        for pair in tqdm(train_h):\n",
    "            train_data.append([pair['id1'],pair['id2'],float(pair['estimated_distance'])])\n",
    "    print(\"Train Data loaded!!\")\n",
    "    feature_extraction_file(train_data,p.replace(\"@.csv\",\"-distance.csv\"),fps_train)\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"*\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f624064-6942-4840-8bad-9b29494dde7e",
   "metadata": {},
   "source": [
    "### Wifi Distance Estimation Model - XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5be26b8-d7c8-4abe-a351-949d7b6b0e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Derin √ñƒürenme modeli i√ßin\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def load_trained_model(model_name):\n",
    "    \"\"\"Eƒüitilmi≈ü modeli y√ºkler\"\"\"\n",
    "    try:\n",
    "        if model_name == \"ANN\":\n",
    "            model_path = f\"regression_models/{model_name}_model.h5\"\n",
    "            model = load_model(model_path)\n",
    "        else:\n",
    "            model_path = f\"regression_models/{model_name}_model.pkl\"\n",
    "            with open(model_path, \"rb\") as f:\n",
    "                model = pickle.load(f)\n",
    "        print(f\"‚úÖ {model_name} modeli ba≈üarƒ±yla y√ºklendi: {model_path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hata: {model_name} modeli y√ºklenemedi! - {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_and_save(model_name, data_path, output_folder=\"prediction_outputs\"):\n",
    "    \"\"\"\n",
    "    Veriyi y√ºkler, model ile tahmin yapar ve sonu√ßlarƒ± kaydeder\n",
    "    \n",
    "    Parametreler:\n",
    "    - model_name: Kullanƒ±lacak model adƒ± (√∂rn: \"BR\", \"XGB\", \"ANN\")\n",
    "    - data_path: Tahmin yapƒ±lacak CSV dosyasƒ±nƒ±n yolu\n",
    "    - output_folder: Sonu√ßlarƒ±n kaydedileceƒüi klas√∂r\n",
    "    \"\"\"\n",
    "    \n",
    "    # √áƒ±ktƒ± klas√∂r√ºn√º olu≈ütur\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"üìä Model: {model_name}\")\n",
    "    print(f\"üìÅ Veri: {data_path}\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # 1. Modeli Y√ºkle\n",
    "    model = load_trained_model(model_name)\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # 2. Veriyi Y√ºkle\n",
    "    try:\n",
    "        data_df = pd.read_csv(data_path)\n",
    "        print(f\"‚úÖ Veri ba≈üarƒ±yla y√ºklendi: {data_df.shape[0]} satƒ±r, {data_df.shape[1]} s√ºtun\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hata: Veri y√ºklenemedi! - {e}\")\n",
    "        return\n",
    "    \n",
    "    # 3. √ñzellik √ßƒ±karƒ±mƒ± (eƒüer 'real' s√ºtunu varsa kaldƒ±r)\n",
    "    if 'real' in data_df.columns:\n",
    "        X_data = data_df.drop('real', axis=1)\n",
    "        has_real_values = True\n",
    "        real_values = data_df['real']\n",
    "    else:\n",
    "        X_data = data_df\n",
    "        has_real_values = False\n",
    "        real_values = None\n",
    "    \n",
    "    # 4. Tahmin Yap\n",
    "    print(\"üîÆ Tahmin yapƒ±lƒ±yor...\")\n",
    "    try:\n",
    "        if model_name == \"ANN\":\n",
    "            predictions = model.predict(X_data).flatten()\n",
    "        else:\n",
    "            predictions = model.predict(X_data)\n",
    "        print(f\"‚úÖ Tahmin tamamlandƒ±! {len(predictions)} adet tahmin √ºretildi.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Hata: Tahmin yapƒ±lamadƒ±! - {e}\")\n",
    "        return\n",
    "    \n",
    "    # 5. Sonu√ßlarƒ± Hazƒ±rla\n",
    "    result_df = pd.DataFrame()\n",
    "    \n",
    "    if has_real_values:\n",
    "        result_df['real'] = real_values\n",
    "        result_df['estimated'] = predictions\n",
    "        result_df['error'] = result_df['real'] - result_df['estimated']\n",
    "        result_df['absolute_error'] = np.abs(result_df['error'])\n",
    "        \n",
    "        # Hata istatistikleri\n",
    "        mae = np.mean(result_df['absolute_error'])\n",
    "        rmse = np.sqrt(np.mean(result_df['error']**2))\n",
    "        print(f\"\\nüìà Performans Metrikleri:\")\n",
    "        print(f\"   MAE (Ortalama Mutlak Hata): {mae:.4f}\")\n",
    "        print(f\"   RMSE (K√∂k Ortalama Kare Hata): {rmse:.4f}\")\n",
    "    else:\n",
    "        result_df['estimated'] = predictions\n",
    "    \n",
    "    # 6. Sonu√ßlarƒ± Kaydet\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_filename =data_path.replace(\"._rss-distance.csv\",\"_predictions.csv\")\n",
    "    \n",
    "    result_df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\n‚úÖ Sonu√ßlar kaydedildi: {output_filename}\")\n",
    "    print(f\"\\n{'='*50}\\n\")\n",
    "    \n",
    "    return result_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0625a5bc-0d7a-4427-94bc-a0a81a1d7ca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./data/4/trainingData._rss-distance.csv',\n",
       " './data/4/validationData._rss-distance.csv']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_add=find_the_way(\"./data/4/\",\"rss-distance.csv\")\n",
    "files_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "313e8b6a-ebd3-4317-89ce-04b327fa1248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üìä Model: XGB\n",
      "üìÅ Veri: ./data/4/trainingData._rss-distance.csv\n",
      "==================================================\n",
      "\n",
      "‚úÖ XGB modeli ba≈üarƒ±yla y√ºklendi: regression_models/XGB_model.pkl\n",
      "‚úÖ Veri ba≈üarƒ±yla y√ºklendi: 67588605 satƒ±r, 10 s√ºtun\n",
      "üîÆ Tahmin yapƒ±lƒ±yor...\n",
      "‚úÖ Tahmin tamamlandƒ±! 67588605 adet tahmin √ºretildi.\n",
      "\n",
      "üìà Performans Metrikleri:\n",
      "   MAE (Ortalama Mutlak Hata): 35.7442\n",
      "   RMSE (K√∂k Ortalama Kare Hata): 58.7216\n",
      "\n",
      "‚úÖ Sonu√ßlar kaydedildi: ./data/4/trainingData_predictions.csv\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "üéâ T√ºm tahminler tamamlandƒ±!\n",
      "\n",
      "==================================================\n",
      "üìä Model: XGB\n",
      "üìÅ Veri: ./data/4/validationData._rss-distance.csv\n",
      "==================================================\n",
      "\n",
      "‚úÖ XGB modeli ba≈üarƒ±yla y√ºklendi: regression_models/XGB_model.pkl\n",
      "‚úÖ Veri ba≈üarƒ±yla y√ºklendi: 174416 satƒ±r, 10 s√ºtun\n",
      "üîÆ Tahmin yapƒ±lƒ±yor...\n",
      "‚úÖ Tahmin tamamlandƒ±! 174416 adet tahmin √ºretildi.\n",
      "\n",
      "üìà Performans Metrikleri:\n",
      "   MAE (Ortalama Mutlak Hata): 27.9444\n",
      "   RMSE (K√∂k Ortalama Kare Hata): 46.1992\n",
      "\n",
      "‚úÖ Sonu√ßlar kaydedildi: ./data/4/validationData_predictions.csv\n",
      "\n",
      "==================================================\n",
      "\n",
      "\n",
      "üéâ T√ºm tahminler tamamlandƒ±!\n"
     ]
    }
   ],
   "source": [
    "for i in files_add:\n",
    "    folder_path = os.path.dirname(os.path.abspath(i))\n",
    "    \n",
    "    # Kullanƒ±labilir modeller\n",
    "    available_models = [ \"XGB\"]  # \"DTR\", \"KNN\", \"LR\", \"ANN\" ekleyebilirsiniz\n",
    "    \n",
    "    # Tahmin yapƒ±lacak veri dosyasƒ±\n",
    "    data_file = i\n",
    "    \n",
    "    # Her model i√ßin tahmin yap\n",
    "    for model_name in available_models:\n",
    "        predict_and_save(\n",
    "            model_name=model_name,\n",
    "            data_path=data_file,\n",
    "            output_folder=folder_path\n",
    "        )\n",
    "    \n",
    "    print(\"\\nüéâ T√ºm tahminler tamamlandƒ±!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b6f960-b7d9-40c2-8595-6df7e97f6852",
   "metadata": {},
   "source": [
    "# Create and move distance files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1785606-98dc-4db1-8780-6fe0bed886b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "real=pd.read_csv(\"./data/4/validationData._rss@.csv\")\n",
    "est=pd.read_csv(\"./data/4/validationData_predictions.csv\")\n",
    "real[\"estimated_distance\"]=est[\"estimated\"].values\n",
    "real.to_csv(\"./data/4/validationData_WDE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a924b2d-3ad3-4fa2-92d8-6e05cfde6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "real=pd.read_csv(\"./data/4/trainingData._rss@.csv\")\n",
    "est=pd.read_csv(\"./data/4/trainingData_predictions.csv\")\n",
    "real[\"estimated_distance\"]=est[\"estimated\"].values\n",
    "real.to_csv(\"./data/4/trainingData_WDE.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e66fa322-497a-4d38-abb3-0bfca7cf5132",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders=[\"TrainingGeo\", \"TrainingWDE\" , \"ValidationGeo\", \"ValidationWDE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8756dc1-7797-4b17-9f7e-ebda59072448",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder_name in folders:\n",
    "    try:\n",
    "        os.mkdir(folder_name)\n",
    "    except FileExistsError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d2edc-94d9-49a0-82ad-d13c39cb680e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e67fc19c-df70-4b6e-bc9f-074517f3912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/4/trainingData.csv TrainingGeo/data.csv\n",
      "./data/4/trainingData._rss@.csv TrainingGeo/data_distances.csv\n",
      "./data/4/validationData.csv ValidationGeo/data.csv\n",
      "./data/4/validationData._rss@.csv ValidationGeo/data_distances.csv\n",
      "./data/4/trainingData.csv TrainingWDE/data.csv\n",
      "./data/4/trainingData_WDE.csv TrainingWDE/data_distances.csv\n",
      "./data/4/validationData.csv ValidationWDE/data.csv\n",
      "./data/4/validationData_WDE.csv ValidationWDE/data_distances.csv\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "copies=[['./data/4/trainingData.csv', \"TrainingGeo/data.csv\"],  ['./data/4/trainingData._rss@.csv', \"TrainingGeo/data_distances.csv\"],\n",
    "['./data/4/validationData.csv', \"ValidationGeo/data.csv\"],  ['./data/4/validationData._rss@.csv', \"ValidationGeo/data_distances.csv\"],\n",
    "['./data/4/trainingData.csv', \"TrainingWDE/data.csv\"],  ['./data/4/trainingData_WDE.csv', \"TrainingWDE/data_distances.csv\"],\n",
    "['./data/4/validationData.csv', \"ValidationWDE/data.csv\"],  ['./data/4/validationData_WDE.csv', \"ValidationWDE/data_distances.csv\"]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for c in copies:\n",
    "    source = c[0]\n",
    "    destination = c[1]\n",
    "    shutil.copy2(source, destination)\n",
    "    print(source, destination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
