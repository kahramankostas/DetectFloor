{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8887dc81-8f61-4dfc-9a36-e82587aa7710",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths=folders= [\"./ValidationGeo\", \"./ValidationWDE\", \"./TrainingGeo\", \"./TrainingWDE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05a03f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2  Setting Random Seeds\n",
    "seed_value=0\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "from sklearn.metrics import calinski_harabasz_score\n",
    "\n",
    "# 2.5  Best result\n",
    "from sklearn import cluster\n",
    "import time\n",
    "\n",
    "\n",
    "def cluster_embeddings_optimized(embeddings, min_clusters=3, max_clusters=20):\n",
    "    # Tensor kontrolü\n",
    "    if torch.is_tensor(embeddings):\n",
    "        embeddings_np = embeddings.detach().cpu().numpy()\n",
    "    else:\n",
    "        embeddings_np = embeddings\n",
    "\n",
    "    best_score = -1\n",
    "    best_k = min_clusters # Varsayılan başlangıç\n",
    "    \n",
    "    print(f\"En iyi küme sayısı aranıyor ({min_clusters}-{max_clusters} arası)...\")\n",
    "    \n",
    "    # Aramaya min_clusters'dan başla\n",
    "    search_range = range(min_clusters, min(max_clusters + 1, len(embeddings_np)))\n",
    "    \n",
    "    for k in search_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(embeddings_np)\n",
    "        \n",
    "        # Calinski-Harabasz kullanıyoruz (Daha yüksek k sayılarını sever)\n",
    "        score = calinski_harabasz_score(embeddings_np, labels)\n",
    "        \n",
    "        # İstersen skorları yazdırıp görebilirsin\n",
    "        # print(f\"k={k}, Score={score:.4f}\")\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_k = k\n",
    "            \n",
    "    print(f\"Seçilen küme sayısı: {best_k} (Score: {best_score:.4f})\")\n",
    "    \n",
    "    # Final kümeleme\n",
    "    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(embeddings_np)\n",
    "    \n",
    "    communities = {}\n",
    "    for label in np.unique(labels):\n",
    "        communities[int(label)] = np.where(labels == label)[0].tolist()\n",
    "    \n",
    "    return communities\n",
    "\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import leidenalg\n",
    "import igraph as ig\n",
    "import time\n",
    "import concurrent.futures\n",
    "import sys\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def run_algorithm(G_ig, algorithm_name):\n",
    "    \"\"\"\n",
    "    Runs the specified community detection algorithm and returns the results\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        if algorithm_name == \"leiden\":\n",
    "            partition = leidenalg.find_partition(G_ig, leidenalg.ModularityVertexPartition)\n",
    "        elif algorithm_name == \"louvain\":\n",
    "            partition = G_ig.community_multilevel(weights='weight' if 'weight' in G_ig.edge_attributes() else None)\n",
    "        elif algorithm_name == \"label_propagation\":\n",
    "            partition = G_ig.community_label_propagation()\n",
    "        elif algorithm_name == \"fast_greedy\":\n",
    "            partition = G_ig.community_fastgreedy().as_clustering()\n",
    "        elif algorithm_name == \"infomap\":\n",
    "            partition = G_ig.community_infomap()\n",
    "        elif algorithm_name == \"edge_betweenness\":\n",
    "            # Edge betweenness is very slow for large graphs\n",
    "            # Add a safety check here\n",
    "            if G_ig.vcount() > 1000:\n",
    "                print(f\"WARNING: Edge Betweenness algorithm is very slow for graphs with {G_ig.vcount()} nodes.\")\n",
    "                print(\"Press 'e' to skip, or any other key to continue:\")\n",
    "                choice = input().lower()\n",
    "                if choice == 'e':\n",
    "                    return None, None\n",
    "            partition = G_ig.community_edge_betweenness().as_clustering()\n",
    "        else:\n",
    "            print(f\"Unknown algorithm: {algorithm_name}\")\n",
    "            return None, None\n",
    "            \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        # Convert results to dictionary\n",
    "        community_dict = {}\n",
    "        for i, community in enumerate(partition):\n",
    "            community_dict[i] = list(community)\n",
    "            \n",
    "        return community_dict, elapsed_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while running {algorithm_name}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def save_communities_to_csv(communities, algorithm_name):\n",
    "    \"\"\"\n",
    "    Saves community results to a CSV file\n",
    "    \"\"\"\n",
    "    if not communities:\n",
    "        return False\n",
    "    \n",
    "    # Check/create output directory\n",
    "    output_dir = f\"./{p}/community_results/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Create filename\n",
    "    filename = os.path.join(output_dir, f\"{algorithm_name.lower()}_communities.csv\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            \n",
    "            # Write each community as a row\n",
    "            for comm_id, members in communities.items():\n",
    "                row = members\n",
    "                writer.writerow(row)\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error while saving CSV file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def print_communities(communities, algorithm_name, elapsed_time):\n",
    "    \"\"\"\n",
    "    Prints community results and saves them to a CSV file\n",
    "    \"\"\"\n",
    "    if not communities:\n",
    "        print(f\"\\n{algorithm_name} results could not be obtained.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\n{algorithm_name} Results (Time: {elapsed_time:.2f} seconds):\")\n",
    "    print(f\"Total of {len(communities)} communities detected.\")\n",
    "    \n",
    "    # Show only the first 5 communities (in case there are too many)\n",
    "    for i, (comm_id, members) in enumerate(communities.items()):\n",
    "        if i < 5:\n",
    "            print(f\"Community {comm_id}: {len(members)} members\")\n",
    "        else:\n",
    "            print(\"...\")\n",
    "            break\n",
    "    \n",
    "    # Find the largest community\n",
    "    max_community = max(communities.items(), key=lambda x: len(x[1]))\n",
    "    print(f\"Largest community: Community {max_community[0]} ({len(max_community[1])} members)\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    if save_communities_to_csv(communities, algorithm_name):\n",
    "        print(f\"Community results saved to {algorithm_name.lower()}_communities.csv\")\n",
    "    else:\n",
    "        print(f\"WARNING: {algorithm_name} results could not be saved to CSV.\")\n",
    "\n",
    "def main():\n",
    "    # Check/create output directory\n",
    "    output_dir = f\"./{p}/community_results/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"'{output_dir}' directory created.\")\n",
    "    \n",
    "    # Load the graph\n",
    "    print(\"Loading graph...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        G_nx = nx.read_adjlist(f\"./{p}/my.adjlist\")\n",
    "        \n",
    "        # Print graph stats\n",
    "        print(f\"Graph properties:\")\n",
    "        print(f\"Number of nodes: {G_nx.number_of_nodes()}\")\n",
    "        print(f\"Number of edges: {G_nx.number_of_edges()}\")\n",
    "        \n",
    "        # Convert to undirected if needed\n",
    "        if nx.is_directed(G_nx):\n",
    "            G_nx = G_nx.to_undirected()\n",
    "        \n",
    "        # Convert NetworkX graph to iGraph\n",
    "        G_ig = ig.Graph.from_networkx(G_nx)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Graph loaded (Time: {elapsed_time:.2f} seconds)\")\n",
    "        \n",
    "        # Choose algorithms to run\n",
    "        algorithms = [\"leiden\", \"louvain\", \"label_propagation\", \"fast_greedy\", \"infomap\"]\n",
    "        \n",
    "        # Skip edge_betweenness for large graphs\n",
    "        if G_ig.vcount() <= 1000:\n",
    "            algorithms.append(\"edge_betweenness\")\n",
    "        else:\n",
    "            print(\"\\nNOTE: Edge Betweenness algorithm will not be run due to large graph size.\")\n",
    "        \n",
    "        print(\"\\nRunning community detection algorithms...\")\n",
    "        \n",
    "        for algorithm in algorithms:\n",
    "            print(f\"\\nRunning {algorithm.capitalize()} algorithm...\")\n",
    "            communities, elapsed_time = run_algorithm(G_ig, algorithm)\n",
    "            if communities:\n",
    "                print_communities(communities, algorithm.capitalize(), elapsed_time)\n",
    "                \n",
    "        # Alternatively, for parallel execution:\n",
    "        \"\"\"\n",
    "        with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            futures = {executor.submit(run_algorithm, G_ig, alg): alg for alg in algorithms}\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "                algorithm = futures[future]\n",
    "                try:\n",
    "                    communities, elapsed_time = future.result()\n",
    "                    if communities:\n",
    "                        print_communities(communities, algorithm.capitalize(), elapsed_time)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while running {algorithm}: {str(e)}\")\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\nAll tasks completed.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c3bd37-fef5-4c1e-b13f-10de29b4bb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174416it [00:01, 159205.12it/s]\n",
      "459it [00:00, 159224.67it/s]\n",
      "988it [00:00, 145178.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 174416/174416 [00:00<00:00, 374803.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 459/459 [00:00<00:00, 230671.64it/s]\n",
      "\n",
      "Computing transition probabilities:   0%|          | 0/123 [00:00<?, ?it/s]\n",
      "Computing transition probabilities:   7%|6         | 8/123 [00:00<00:01, 75.01it/s]\n",
      "Computing transition probabilities:  14%|#3        | 17/123 [00:00<00:01, 80.25it/s]\n",
      "Computing transition probabilities:  21%|##1       | 26/123 [00:00<00:01, 76.22it/s]\n",
      "Computing transition probabilities:  28%|##7       | 34/123 [00:00<00:01, 73.98it/s]\n",
      "Computing transition probabilities:  34%|###4      | 42/123 [00:00<00:01, 68.17it/s]\n",
      "Computing transition probabilities:  41%|####      | 50/123 [00:00<00:01, 69.46it/s]\n",
      "Computing transition probabilities:  48%|####7     | 59/123 [00:00<00:00, 74.78it/s]\n",
      "Computing transition probabilities:  54%|#####4    | 67/123 [00:00<00:00, 74.43it/s]\n",
      "Computing transition probabilities:  62%|######1   | 76/123 [00:01<00:00, 76.93it/s]\n",
      "Computing transition probabilities:  70%|######9   | 86/123 [00:01<00:00, 82.55it/s]\n",
      "Computing transition probabilities:  81%|########1 | 100/123 [00:01<00:00, 97.08it/s]\n",
      "Computing transition probabilities:  93%|#########2| 114/123 [00:01<00:00, 108.25it/s]\n",
      "Computing transition probabilities: 100%|##########| 123/123 [00:01<00:00, 88.17it/s] \n",
      "\n",
      "Generating walks (CPU: 1):   0%|          | 0/100 [00:00<?, ?it/s]\n",
      "Generating walks (CPU: 1):   5%|5         | 5/100 [00:00<00:01, 48.39it/s]\n",
      "Generating walks (CPU: 1):  10%|#         | 10/100 [00:00<00:02, 42.70it/s]\n",
      "Generating walks (CPU: 1):  15%|#5        | 15/100 [00:00<00:02, 39.41it/s]\n",
      "Generating walks (CPU: 1):  19%|#9        | 19/100 [00:00<00:02, 38.08it/s]\n",
      "Generating walks (CPU: 1):  24%|##4       | 24/100 [00:00<00:01, 38.89it/s]\n",
      "Generating walks (CPU: 1):  28%|##8       | 28/100 [00:00<00:01, 38.90it/s]\n",
      "Generating walks (CPU: 1):  32%|###2      | 32/100 [00:00<00:01, 38.99it/s]\n",
      "Generating walks (CPU: 1):  37%|###7      | 37/100 [00:00<00:01, 39.97it/s]\n",
      "Generating walks (CPU: 1):  42%|####2     | 42/100 [00:01<00:01, 40.62it/s]\n",
      "Generating walks (CPU: 1):  47%|####6     | 47/100 [00:01<00:01, 40.73it/s]\n",
      "Generating walks (CPU: 1):  52%|#####2    | 52/100 [00:01<00:01, 40.85it/s]\n",
      "Generating walks (CPU: 1):  57%|#####6    | 57/100 [00:01<00:01, 41.31it/s]\n",
      "Generating walks (CPU: 1):  62%|######2   | 62/100 [00:01<00:00, 40.83it/s]\n",
      "Generating walks (CPU: 1):  67%|######7   | 67/100 [00:01<00:00, 40.72it/s]\n",
      "Generating walks (CPU: 1):  72%|#######2  | 72/100 [00:01<00:00, 41.07it/s]\n",
      "Generating walks (CPU: 1):  77%|#######7  | 77/100 [00:01<00:00, 40.04it/s]\n",
      "Generating walks (CPU: 1):  82%|########2 | 82/100 [00:02<00:00, 39.94it/s]\n",
      "Generating walks (CPU: 1):  86%|########6 | 86/100 [00:02<00:00, 37.97it/s]\n",
      "Generating walks (CPU: 1):  90%|######### | 90/100 [00:02<00:00, 35.75it/s]\n",
      "Generating walks (CPU: 1):  94%|#########3| 94/100 [00:02<00:00, 35.63it/s]\n",
      "Generating walks (CPU: 1):  98%|#########8| 98/100 [00:02<00:00, 35.38it/s]\n",
      "Generating walks (CPU: 1): 100%|##########| 100/100 [00:02<00:00, 38.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Score: 1786.7479)\n",
      "shape of vector file:  (123, 33)\n",
      "[[ 1.0400000e+02  2.2637676e-01  6.3999510e-03 ... -1.3807258e-01\n",
      "   2.2428058e-01 -3.5540980e-01]\n",
      " [ 1.0300000e+02  3.4622487e-01  8.9824370e-02 ...  2.7656011e-02\n",
      "  -7.7293360e-02 -9.2037074e-02]\n",
      " [ 7.6000000e+01  4.0894390e-01 -5.9875645e-02 ...  1.2728234e-01\n",
      "  -1.1477421e-01 -1.3443314e-01]\n",
      " ...\n",
      " [ 2.6000000e+01  4.5844954e-01  1.3734044e-01 ... -2.9272008e-01\n",
      "   4.0598315e-01 -1.5071204e-01]\n",
      " [ 2.7000000e+01  4.8750423e-02  1.8444265e-01 ... -5.1432200e-01\n",
      "   6.9531040e-01 -1.1705779e-01]\n",
      " [ 4.3000000e+01  2.1000692e-01 -1.8139179e-01 ... -7.9801030e-01\n",
      "   5.3324187e-01  2.4091307e-02]]\n",
      "KMeans(n_clusters=20, random_state=10) 0.010541915893554688\n",
      "././ValidationGeo/community_results/Node2Vec_communities.csv file is ready!\n",
      "Loading graph...\n",
      "Graph properties:\n",
      "Number of nodes: 123\n",
      "Number of edges: 3694\n",
      "Graph loaded (Time: 0.01 seconds)\n",
      "\n",
      "Running community detection algorithms...\n",
      "\n",
      "Running Leiden algorithm...\n",
      "\n",
      "Leiden Results (Time: 0.00 seconds):\n",
      "Total of 3 communities detected.\n",
      "Community 0: 63 members\n",
      "Community 1: 58 members\n",
      "Community 2: 2 members\n",
      "Largest community: Community 0 (63 members)\n",
      "Community results saved to leiden_communities.csv\n",
      "\n",
      "Running Louvain algorithm...\n",
      "\n",
      "Louvain Results (Time: 0.00 seconds):\n",
      "Total of 2 communities detected.\n",
      "Community 0: 55 members\n",
      "Community 1: 68 members\n",
      "Largest community: Community 1 (68 members)\n",
      "Community results saved to louvain_communities.csv\n",
      "\n",
      "Running Label_propagation algorithm...\n",
      "\n",
      "Label_propagation Results (Time: 0.00 seconds):\n",
      "Total of 1 communities detected.\n",
      "Community 0: 123 members\n",
      "Largest community: Community 0 (123 members)\n",
      "Community results saved to label_propagation_communities.csv\n",
      "\n",
      "Running Fast_greedy algorithm...\n",
      "\n",
      "Fast_greedy Results (Time: 0.00 seconds):\n",
      "Total of 2 communities detected.\n",
      "Community 0: 64 members\n",
      "Community 1: 59 members\n",
      "Largest community: Community 0 (64 members)\n",
      "Community results saved to fast_greedy_communities.csv\n",
      "\n",
      "Running Infomap algorithm...\n",
      "\n",
      "Infomap Results (Time: 0.02 seconds):\n",
      "Total of 1 communities detected.\n",
      "Community 0: 123 members\n",
      "Largest community: Community 0 (123 members)\n",
      "Community results saved to infomap_communities.csv\n",
      "\n",
      "Running Edge_betweenness algorithm...\n",
      "\n",
      "Edge_betweenness Results (Time: 11.49 seconds):\n",
      "Total of 36 communities detected.\n",
      "Community 0: 88 members\n",
      "Community 1: 1 members\n",
      "Community 2: 1 members\n",
      "Community 3: 1 members\n",
      "Community 4: 1 members\n",
      "...\n",
      "Largest community: Community 0 (88 members)\n",
      "Community results saved to edge_betweenness_communities.csv\n",
      "\n",
      "All tasks completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "174416it [00:00, 195591.93it/s]\n",
      "459it [00:00, 457506.07it/s]\n",
      "988it [00:00, 247076.82it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████| 174416/174416 [00:00<00:00, 394708.82it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 459/459 [00:00<00:00, 230726.93it/s]\n",
      "\n",
      "Computing transition probabilities:   0%|          | 0/123 [00:00<?, ?it/s]\n",
      "Computing transition probabilities:   7%|6         | 8/123 [00:00<00:01, 71.58it/s]\n",
      "Computing transition probabilities:  15%|#4        | 18/123 [00:00<00:01, 83.33it/s]\n",
      "Computing transition probabilities:  22%|##1       | 27/123 [00:00<00:01, 77.51it/s]\n",
      "Computing transition probabilities:  29%|##9       | 36/123 [00:00<00:01, 78.21it/s]\n",
      "Computing transition probabilities:  36%|###5      | 44/123 [00:00<00:01, 74.00it/s]\n",
      "Computing transition probabilities:  43%|####3     | 53/123 [00:00<00:00, 77.76it/s]\n",
      "Computing transition probabilities:  50%|#####     | 62/123 [00:00<00:00, 79.86it/s]\n",
      "Computing transition probabilities:  58%|#####7    | 71/123 [00:00<00:00, 77.68it/s]\n",
      "Computing transition probabilities:  66%|######5   | 81/123 [00:01<00:00, 83.67it/s]\n",
      "Computing transition probabilities:  75%|#######4  | 92/123 [00:01<00:00, 91.00it/s]\n",
      "Computing transition probabilities:  87%|########6 | 107/123 [00:01<00:00, 108.07it/s]\n",
      "Computing transition probabilities:  98%|#########7| 120/123 [00:01<00:00, 114.40it/s]\n",
      "Computing transition probabilities: 100%|##########| 123/123 [00:01<00:00, 92.92it/s] \n",
      "\n",
      "Generating walks (CPU: 1):   0%|          | 0/100 [00:00<?, ?it/s]\n",
      "Generating walks (CPU: 1):   6%|6         | 6/100 [00:00<00:01, 52.92it/s]\n",
      "Generating walks (CPU: 1):  12%|#2        | 12/100 [00:00<00:01, 46.40it/s]\n",
      "Generating walks (CPU: 1):  17%|#7        | 17/100 [00:00<00:01, 45.25it/s]\n",
      "Generating walks (CPU: 1):  22%|##2       | 22/100 [00:00<00:01, 46.03it/s]\n",
      "Generating walks (CPU: 1):  27%|##7       | 27/100 [00:00<00:01, 44.03it/s]\n",
      "Generating walks (CPU: 1):  32%|###2      | 32/100 [00:00<00:01, 43.03it/s]\n",
      "Generating walks (CPU: 1):  37%|###7      | 37/100 [00:00<00:01, 43.37it/s]\n",
      "Generating walks (CPU: 1):  42%|####2     | 42/100 [00:00<00:01, 43.41it/s]\n",
      "Generating walks (CPU: 1):  47%|####6     | 47/100 [00:01<00:01, 42.28it/s]\n",
      "Generating walks (CPU: 1):  52%|#####2    | 52/100 [00:01<00:01, 41.20it/s]\n",
      "Generating walks (CPU: 1):  57%|#####6    | 57/100 [00:01<00:01, 39.59it/s]\n",
      "Generating walks (CPU: 1):  62%|######2   | 62/100 [00:01<00:00, 40.46it/s]\n",
      "Generating walks (CPU: 1):  67%|######7   | 67/100 [00:01<00:00, 40.60it/s]\n",
      "Generating walks (CPU: 1):  72%|#######2  | 72/100 [00:01<00:00, 40.18it/s]\n",
      "Generating walks (CPU: 1):  77%|#######7  | 77/100 [00:01<00:00, 41.57it/s]\n",
      "Generating walks (CPU: 1):  82%|########2 | 82/100 [00:01<00:00, 42.06it/s]\n",
      "Generating walks (CPU: 1):  87%|########7 | 87/100 [00:02<00:00, 40.66it/s]\n",
      "Generating walks (CPU: 1):  92%|#########2| 92/100 [00:02<00:00, 41.53it/s]\n",
      "Generating walks (CPU: 1):  97%|#########7| 97/100 [00:02<00:00, 39.85it/s]\n",
      "Generating walks (CPU: 1): 100%|##########| 100/100 [00:02<00:00, 41.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Score: 1786.7479)\n",
      "shape of vector file:  (123, 33)\n",
      "[[ 1.0400000e+02  2.2637676e-01  6.3999510e-03 ... -1.3807258e-01\n",
      "   2.2428058e-01 -3.5540980e-01]\n",
      " [ 1.0300000e+02  3.4622487e-01  8.9824370e-02 ...  2.7656011e-02\n",
      "  -7.7293360e-02 -9.2037074e-02]\n",
      " [ 7.6000000e+01  4.0894390e-01 -5.9875645e-02 ...  1.2728234e-01\n",
      "  -1.1477421e-01 -1.3443314e-01]\n",
      " ...\n",
      " [ 2.6000000e+01  4.5844954e-01  1.3734044e-01 ... -2.9272008e-01\n",
      "   4.0598315e-01 -1.5071204e-01]\n",
      " [ 2.7000000e+01  4.8750423e-02  1.8444265e-01 ... -5.1432200e-01\n",
      "   6.9531040e-01 -1.1705779e-01]\n",
      " [ 4.3000000e+01  2.1000692e-01 -1.8139179e-01 ... -7.9801030e-01\n",
      "   5.3324187e-01  2.4091307e-02]]\n",
      "KMeans(n_clusters=20, random_state=10) 0.012368917465209961\n",
      "././ValidationWDE/community_results/Node2Vec_communities.csv file is ready!\n",
      "Loading graph...\n",
      "Graph properties:\n",
      "Number of nodes: 123\n",
      "Number of edges: 3694\n",
      "Graph loaded (Time: 0.01 seconds)\n",
      "\n",
      "Running community detection algorithms...\n",
      "\n",
      "Running Leiden algorithm...\n",
      "\n",
      "Leiden Results (Time: 0.00 seconds):\n",
      "Total of 3 communities detected.\n",
      "Community 0: 57 members\n",
      "Community 1: 55 members\n",
      "Community 2: 11 members\n",
      "Largest community: Community 0 (57 members)\n",
      "Community results saved to leiden_communities.csv\n",
      "\n",
      "Running Louvain algorithm...\n",
      "\n",
      "Louvain Results (Time: 0.00 seconds):\n",
      "Total of 3 communities detected.\n",
      "Community 0: 55 members\n",
      "Community 1: 57 members\n",
      "Community 2: 11 members\n",
      "Largest community: Community 1 (57 members)\n",
      "Community results saved to louvain_communities.csv\n",
      "\n",
      "Running Label_propagation algorithm...\n",
      "\n",
      "Label_propagation Results (Time: 0.00 seconds):\n",
      "Total of 2 communities detected.\n",
      "Community 0: 53 members\n",
      "Community 1: 70 members\n",
      "Largest community: Community 1 (70 members)\n",
      "Community results saved to label_propagation_communities.csv\n",
      "\n",
      "Running Fast_greedy algorithm...\n",
      "\n",
      "Fast_greedy Results (Time: 0.00 seconds):\n",
      "Total of 2 communities detected.\n",
      "Community 0: 64 members\n",
      "Community 1: 59 members\n",
      "Largest community: Community 0 (64 members)\n",
      "Community results saved to fast_greedy_communities.csv\n",
      "\n",
      "Running Infomap algorithm...\n",
      "\n",
      "Infomap Results (Time: 0.02 seconds):\n",
      "Total of 1 communities detected.\n",
      "Community 0: 123 members\n",
      "Largest community: Community 0 (123 members)\n",
      "Community results saved to infomap_communities.csv\n",
      "\n",
      "Running Edge_betweenness algorithm...\n",
      "\n",
      "Edge_betweenness Results (Time: 12.35 seconds):\n",
      "Total of 36 communities detected.\n",
      "Community 0: 88 members\n",
      "Community 1: 1 members\n",
      "Community 2: 1 members\n",
      "Community 3: 1 members\n",
      "Community 4: 1 members\n",
      "...\n",
      "Largest community: Community 0 (88 members)\n",
      "Community results saved to edge_betweenness_communities.csv\n",
      "\n",
      "All tasks completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67588605it [06:25, 175289.53it/s]\n",
      "389it [00:00, ?it/s]\n",
      "19878it [00:00, 181946.76it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 67588605/67588605 [03:06<00:00, 362289.84it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████| 389/389 [00:00<?, ?it/s]\n",
      "\n",
      "Computing transition probabilities:   0%|          | 0/59 [00:00<?, ?it/s]\n",
      "Computing transition probabilities:  32%|###2      | 19/59 [00:00<00:00, 183.67it/s]\n",
      "Computing transition probabilities:  64%|######4   | 38/59 [00:00<00:00, 182.69it/s]\n",
      "Computing transition probabilities:  97%|#########6| 57/59 [00:00<00:00, 180.01it/s]\n",
      "Computing transition probabilities: 100%|##########| 59/59 [00:00<00:00, 181.52it/s]\n",
      "\n",
      "Generating walks (CPU: 1):   0%|          | 0/100 [00:00<?, ?it/s]\n",
      "Generating walks (CPU: 1):  14%|#4        | 14/100 [00:00<00:00, 130.72it/s]\n",
      "Generating walks (CPU: 1):  28%|##8       | 28/100 [00:00<00:00, 121.79it/s]\n",
      "Generating walks (CPU: 1):  41%|####1     | 41/100 [00:00<00:00, 121.13it/s]\n",
      "Generating walks (CPU: 1):  54%|#####4    | 54/100 [00:00<00:00, 118.91it/s]\n",
      "Generating walks (CPU: 1):  67%|######7   | 67/100 [00:00<00:00, 121.29it/s]\n",
      "Generating walks (CPU: 1):  80%|########  | 80/100 [00:00<00:00, 116.05it/s]\n",
      "Generating walks (CPU: 1):  92%|#########2| 92/100 [00:00<00:00, 116.18it/s]\n",
      "Generating walks (CPU: 1): 100%|##########| 100/100 [00:00<00:00, 116.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Score: 693.4006)\n",
      "shape of vector file:  (59, 33)\n",
      "[[ 5.80000000e+01 -2.61382980e-02  2.26462870e-01 ...  2.90218800e-01\n",
      "   1.70179620e-02  1.13002160e-01]\n",
      " [ 5.70000000e+01 -6.31274660e-02  2.42057980e-01 ...  3.66515430e-01\n",
      "  -2.03696820e-02  1.07730190e-01]\n",
      " [ 5.00000000e+01  1.07711870e-01  2.79857430e-01 ...  2.16392860e-01\n",
      "   1.44559685e-02  4.11496530e-02]\n",
      " ...\n",
      " [ 3.30000000e+01  1.35819450e-01  4.03025500e-02 ...  4.37330750e-01\n",
      "  -9.59635100e-02  4.06501470e-01]\n",
      " [ 3.60000000e+01  1.05246970e-01  9.58451260e-02 ...  4.83163680e-01\n",
      "  -1.08766690e-01  3.48558340e-01]\n",
      " [ 4.00000000e+00 -2.12246400e-01  1.49387630e-01 ...  4.01545700e-01\n",
      "  -5.18066060e-02 -9.40422200e-03]]\n",
      "KMeans(n_clusters=20, random_state=10) 0.013186454772949219\n",
      "././TrainingGeo/community_results/Node2Vec_communities.csv file is ready!\n",
      "Loading graph...\n",
      "Graph properties:\n",
      "Number of nodes: 59\n",
      "Number of edges: 1132\n",
      "Graph loaded (Time: 0.01 seconds)\n",
      "\n",
      "Running community detection algorithms...\n",
      "\n",
      "Running Leiden algorithm...\n",
      "\n",
      "Leiden Results (Time: 0.00 seconds):\n",
      "Total of 5 communities detected.\n",
      "Community 0: 15 members\n",
      "Community 1: 13 members\n",
      "Community 2: 12 members\n",
      "Community 3: 10 members\n",
      "Community 4: 9 members\n",
      "Largest community: Community 0 (15 members)\n",
      "Community results saved to leiden_communities.csv\n",
      "\n",
      "Running Louvain algorithm...\n",
      "\n",
      "Louvain Results (Time: 0.00 seconds):\n",
      "Total of 5 communities detected.\n",
      "Community 0: 13 members\n",
      "Community 1: 12 members\n",
      "Community 2: 13 members\n",
      "Community 3: 13 members\n",
      "Community 4: 8 members\n",
      "Largest community: Community 0 (13 members)\n",
      "Community results saved to louvain_communities.csv\n",
      "\n",
      "Running Label_propagation algorithm...\n",
      "\n",
      "Label_propagation Results (Time: 0.00 seconds):\n",
      "Total of 59 communities detected.\n",
      "Community 0: 1 members\n",
      "Community 1: 1 members\n",
      "Community 2: 1 members\n",
      "Community 3: 1 members\n",
      "Community 4: 1 members\n",
      "...\n",
      "Largest community: Community 0 (1 members)\n",
      "Community results saved to label_propagation_communities.csv\n",
      "\n",
      "Running Fast_greedy algorithm...\n",
      "\n",
      "Fast_greedy Results (Time: 0.00 seconds):\n",
      "Total of 4 communities detected.\n",
      "Community 0: 17 members\n",
      "Community 1: 12 members\n",
      "Community 2: 6 members\n",
      "Community 3: 24 members\n",
      "Largest community: Community 3 (24 members)\n",
      "Community results saved to fast_greedy_communities.csv\n",
      "\n",
      "Running Infomap algorithm...\n",
      "\n",
      "Infomap Results (Time: 0.00 seconds):\n",
      "Total of 1 communities detected.\n",
      "Community 0: 59 members\n",
      "Largest community: Community 0 (59 members)\n",
      "Community results saved to infomap_communities.csv\n",
      "\n",
      "Running Edge_betweenness algorithm...\n",
      "\n",
      "Edge_betweenness Results (Time: 0.64 seconds):\n",
      "Total of 43 communities detected.\n",
      "Community 0: 1 members\n",
      "Community 1: 1 members\n",
      "Community 2: 1 members\n",
      "Community 3: 1 members\n",
      "Community 4: 1 members\n",
      "...\n",
      "Largest community: Community 6 (17 members)\n",
      "Community results saved to edge_betweenness_communities.csv\n",
      "\n",
      "All tasks completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67588605it [06:16, 179417.72it/s]\n",
      "389it [00:00, 390237.80it/s]\n",
      "19878it [00:00, 204545.48it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████| 67588605/67588605 [02:59<00:00, 375877.38it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 389/389 [00:00<00:00, 431407.79it/s]\n",
      "\n",
      "Computing transition probabilities:   0%|          | 0/59 [00:00<?, ?it/s]\n",
      "Computing transition probabilities:  29%|##8       | 17/59 [00:00<00:00, 167.12it/s]\n",
      "Computing transition probabilities:  66%|######6   | 39/59 [00:00<00:00, 194.44it/s]\n",
      "Computing transition probabilities: 100%|##########| 59/59 [00:00<00:00, 198.68it/s]\n",
      "\n",
      "Generating walks (CPU: 1):   0%|          | 0/100 [00:00<?, ?it/s]\n",
      "Generating walks (CPU: 1):  13%|#3        | 13/100 [00:00<00:00, 126.07it/s]\n",
      "Generating walks (CPU: 1):  26%|##6       | 26/100 [00:00<00:00, 122.36it/s]\n",
      "Generating walks (CPU: 1):  39%|###9      | 39/100 [00:00<00:00, 117.98it/s]\n",
      "Generating walks (CPU: 1):  52%|#####2    | 52/100 [00:00<00:00, 119.16it/s]\n",
      "Generating walks (CPU: 1):  66%|######6   | 66/100 [00:00<00:00, 123.27it/s]\n",
      "Generating walks (CPU: 1):  79%|#######9  | 79/100 [00:00<00:00, 124.75it/s]\n",
      "Generating walks (CPU: 1):  92%|#########2| 92/100 [00:00<00:00, 122.44it/s]\n",
      "Generating walks (CPU: 1): 100%|##########| 100/100 [00:00<00:00, 119.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Score: 693.4006)\n",
      "shape of vector file:  (59, 33)\n",
      "[[ 5.80000000e+01 -2.61382980e-02  2.26462870e-01 ...  2.90218800e-01\n",
      "   1.70179620e-02  1.13002160e-01]\n",
      " [ 5.70000000e+01 -6.31274660e-02  2.42057980e-01 ...  3.66515430e-01\n",
      "  -2.03696820e-02  1.07730190e-01]\n",
      " [ 5.00000000e+01  1.07711870e-01  2.79857430e-01 ...  2.16392860e-01\n",
      "   1.44559685e-02  4.11496530e-02]\n",
      " ...\n",
      " [ 3.30000000e+01  1.35819450e-01  4.03025500e-02 ...  4.37330750e-01\n",
      "  -9.59635100e-02  4.06501470e-01]\n",
      " [ 3.60000000e+01  1.05246970e-01  9.58451260e-02 ...  4.83163680e-01\n",
      "  -1.08766690e-01  3.48558340e-01]\n",
      " [ 4.00000000e+00 -2.12246400e-01  1.49387630e-01 ...  4.01545700e-01\n",
      "  -5.18066060e-02 -9.40422200e-03]]\n",
      "KMeans(n_clusters=20, random_state=10) 0.012492895126342773\n",
      "././TrainingWDE/community_results/Node2Vec_communities.csv file is ready!\n",
      "Loading graph...\n",
      "Graph properties:\n",
      "Number of nodes: 59\n",
      "Number of edges: 1132\n",
      "Graph loaded (Time: 0.01 seconds)\n",
      "\n",
      "Running community detection algorithms...\n",
      "\n",
      "Running Leiden algorithm...\n",
      "\n",
      "Leiden Results (Time: 0.00 seconds):\n",
      "Total of 5 communities detected.\n",
      "Community 0: 18 members\n",
      "Community 1: 13 members\n",
      "Community 2: 11 members\n",
      "Community 3: 9 members\n",
      "Community 4: 8 members\n",
      "Largest community: Community 0 (18 members)\n",
      "Community results saved to leiden_communities.csv\n",
      "\n",
      "Running Louvain algorithm...\n",
      "\n",
      "Louvain Results (Time: 0.00 seconds):\n",
      "Total of 4 communities detected.\n",
      "Community 0: 16 members\n",
      "Community 1: 15 members\n",
      "Community 2: 15 members\n",
      "Community 3: 13 members\n",
      "Largest community: Community 0 (16 members)\n",
      "Community results saved to louvain_communities.csv\n",
      "\n",
      "Running Label_propagation algorithm...\n",
      "\n",
      "Label_propagation Results (Time: 0.00 seconds):\n",
      "Total of 59 communities detected.\n",
      "Community 0: 1 members\n",
      "Community 1: 1 members\n",
      "Community 2: 1 members\n",
      "Community 3: 1 members\n",
      "Community 4: 1 members\n",
      "...\n",
      "Largest community: Community 0 (1 members)\n",
      "Community results saved to label_propagation_communities.csv\n",
      "\n",
      "Running Fast_greedy algorithm...\n",
      "\n",
      "Fast_greedy Results (Time: 0.00 seconds):\n",
      "Total of 4 communities detected.\n",
      "Community 0: 17 members\n",
      "Community 1: 12 members\n",
      "Community 2: 6 members\n",
      "Community 3: 24 members\n",
      "Largest community: Community 3 (24 members)\n",
      "Community results saved to fast_greedy_communities.csv\n",
      "\n",
      "Running Infomap algorithm...\n",
      "\n",
      "Infomap Results (Time: 0.01 seconds):\n",
      "Total of 1 communities detected.\n",
      "Community 0: 59 members\n",
      "Largest community: Community 0 (59 members)\n",
      "Community results saved to infomap_communities.csv\n",
      "\n",
      "Running Edge_betweenness algorithm...\n",
      "\n",
      "Edge_betweenness Results (Time: 0.60 seconds):\n",
      "Total of 43 communities detected.\n",
      "Community 0: 1 members\n",
      "Community 1: 1 members\n",
      "Community 2: 1 members\n",
      "Community 3: 1 members\n",
      "Community 4: 1 members\n",
      "...\n",
      "Largest community: Community 6 (17 members)\n",
      "Community results saved to edge_betweenness_communities.csv\n",
      "\n",
      "All tasks completed.\n"
     ]
    }
   ],
   "source": [
    "for p in paths:\n",
    "    ## 2.3 Loading the data  \n",
    "    import os\n",
    "    import json\n",
    "    import csv\n",
    "    import networkx as nx\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    path_to_data = p\n",
    "    \n",
    "    with open(os.path.join(path_to_data,f\"data_distances.csv\")) as f:\n",
    "        wifi = []\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in tqdm(reader):\n",
    "            wifi.append([line['id1'],line['id2'],float(line['estimated_distance'])])\n",
    "            \n",
    "    with open(os.path.join(path_to_data,f\"data_elevations.csv\")) as f:\n",
    "        elevs = []\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in tqdm(reader):\n",
    "            elevs.append([line['id1'],line['id2']])        \n",
    "    \n",
    "    with open(os.path.join(path_to_data,f\"data_steps.csv\")) as f:\n",
    "        steps = []\n",
    "        reader = csv.DictReader(f)\n",
    "        for line in tqdm(reader):\n",
    "            steps.append([line['id1'],line['id2'],float(line['displacement'])]) \n",
    "            \n",
    "    fp_lookup_path = os.path.join(path_to_data,f\"data_lookup.json\")\n",
    "    \n",
    "    with open(fp_lookup_path) as f:\n",
    "        fp_lookup = json.load(f)\n",
    "\n",
    "\n",
    "    ## 2.3 Generating the Trajectory  graph. \n",
    "    B = nx.Graph()\n",
    "    \n",
    "    # Get all the trajectory ids from the lookup\n",
    "    valid_nodes = set(fp_lookup.values())\n",
    "    \n",
    "    for node in valid_nodes:\n",
    "        B.add_node(node)\n",
    "    \n",
    "    # Either add an edge or append the distance to the edge data\n",
    "    for id1,id2,dist in tqdm(wifi):\n",
    "        if not B.has_edge(fp_lookup[str(id1)], fp_lookup[str(id2)]):\n",
    "            \n",
    "            B.add_edge(fp_lookup[str(id1)], \n",
    "                       fp_lookup[str(id2)], \n",
    "                       ty = \"w\", weight=[dist])\n",
    "        else:\n",
    "            B[fp_lookup[str(id1)]][fp_lookup[str(id2)]]['weight'].append(dist)\n",
    "            \n",
    "    # Compute the mean edge weight\n",
    "    for edge in B.edges(data=True):\n",
    "        B[edge[0]][edge[1]]['weight'] = sum(B[edge[0]][edge[1]]['weight'])/len(B[edge[0]][edge[1]]['weight'])\n",
    "            \n",
    "    # If you have made a wifi connection between trajectories with an elev, delete the edge\n",
    "    for id1,id2 in tqdm(elevs):\n",
    "        if B.has_edge(fp_lookup[str(id1)], fp_lookup[str(id2)]):\n",
    "            B.remove_edge(fp_lookup[str(id1)], \n",
    "                          fp_lookup[str(id2)])\n",
    "    \n",
    "    nx.write_adjlist(B, f\"{p}/my.adjlist\")\n",
    "\n",
    "\n",
    "    # 2.4 Converting nodes to vectors\n",
    "    # A folder named tmp is created. This folder is essential for the node2vec model to use less RAM.\n",
    "    try:\n",
    "        if not os.path.exists(\"tmp\"):\n",
    "            os.makedirs(\"tmp\")\n",
    "    except OSError:\n",
    "        print (\"The folder could not be created!\\n Please manually create the \\\"tmp\\\" folder in the directory\")\n",
    "    \n",
    "    \n",
    "    node=f\"\"\"\n",
    " \n",
    "# importing related modules\n",
    "\n",
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "\n",
    "#importing  adjacency list file as B\n",
    "B = nx.read_adjlist(\"{p}/my.adjlist\")\n",
    "\n",
    "seed_value=0\n",
    "\n",
    "\n",
    "# Specifying the input and hyperparameters of the node2vec model\n",
    "node2vec = Node2Vec(B, dimensions=32, walk_length=15, num_walks=100, workers=1,seed=seed_value,temp_folder = './tmp')  \n",
    "\n",
    "#Assigning/specifying random seeds\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "\n",
    "# creation of the model\n",
    "\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4,seed=seed_value)   \n",
    "\n",
    "\n",
    "# saving the output vector\n",
    "\n",
    "model.wv.save_word2vec_format(\"{p}/vectors.emb\")\n",
    "\n",
    "# save the model\n",
    "model.save(\"{p}/vectorMODEL\")\n",
    "\n",
    "    \"\"\"\n",
    "    f = open(\"node.py\", \"w\")\n",
    "    f.write(node)\n",
    "    f.close()\n",
    "    !python node.py\n",
    "    embeddings = np.loadtxt(f\"{p}/vectors.emb\", skiprows=1)\n",
    "    a=cluster_embeddings_optimized(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # 2.4 Reshaping data\n",
    "    vec = np.loadtxt(f\"{p}/vectors.emb\", skiprows=1)\n",
    "    print(\"shape of vector file: \",vec.shape)\n",
    "    print(vec)\n",
    "    vec=vec[vec[:,0].argsort()]; \n",
    "    vec=vec[0:vec.shape[0],1:vec.shape[1]]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ML_results = []\n",
    "    k_clusters =len(a)\n",
    "    algorithms = {}\n",
    "    algorithms['KMeans'] =cluster.KMeans(n_clusters=k_clusters,random_state=10)\n",
    "    second=time.time()\n",
    "    for model in algorithms.values():\n",
    "        model.fit(vec)\n",
    "        ML_results=list(model.labels_)\n",
    "        print(model,time.time()-second)\n",
    "    \n",
    "\n",
    "\n",
    "    ## 2.6 RESULTS \n",
    "    result={}\n",
    "    for ii,i in enumerate(set(fp_lookup.values())):\n",
    "        result[i]=ML_results[ii]\n",
    "            \n",
    "        \n",
    "    ters={}\n",
    "    for i in result:\n",
    "        if result[i] not in ters:\n",
    "            ters[result[i]]=[]\n",
    "            ters[result[i]].append(i)\n",
    "        else:\n",
    "            ters[result[i]].append(i)\n",
    "            \n",
    "            \n",
    "            \n",
    "    final_results=[]\n",
    "    for i in ters:\n",
    "        final_results.append(ters[i])\n",
    "\n",
    "        \n",
    "    output_dir = f\"./{p}/community_results/\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    name=f\"./{p}/community_results/Node2Vec_communities.csv\"    \n",
    "    with open(name, \"w\", newline='') as f:\n",
    "        csv_writer = csv.writer(f)\n",
    "        csv_writer.writerows(final_results)\n",
    "    print(name, \"file is ready!\")\n",
    "\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bef08-d574-409b-9318-7c550f0288cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad444ea-7762-4a6d-9bec-c3522c153c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
