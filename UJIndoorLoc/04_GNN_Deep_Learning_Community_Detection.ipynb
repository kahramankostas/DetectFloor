{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60daea7d-da5e-48b7-8ee6-a189d1c66794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7194e1d6-7b2b-41c0-add9-56fc2ddd8e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graf yükleniyor...\n",
      "Düğüm sayısı: 123\n",
      "\n",
      "GCN modeli çalıştırılıyor...\n",
      "Epoch 50/200, Loss: 1.3471\n",
      "Epoch 100/200, Loss: 1.3694\n",
      "Epoch 150/200, Loss: 1.3228\n",
      "Epoch 200/200, Loss: 1.3393\n",
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Score: 2332.4651)\n",
      "\n",
      "GCN_GNN Tamamlandı (52.77 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Embeddingler kaydedildi: ./ValidationGeo/community_results\\gcn_gnn_embeddings.csv\n",
      "\n",
      "GAT modeli çalıştırılıyor...\n",
      "Epoch 50/200, Loss: 22.8444\n",
      "Epoch 100/200, Loss: 19.7017\n",
      "Epoch 150/200, Loss: 20.0336\n",
      "Epoch 200/200, Loss: 25.5380\n",
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Score: 2891.1374)\n",
      "\n",
      "GAT_GNN Tamamlandı (56.34 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Embeddingler kaydedildi: ./ValidationGeo/community_results\\gat_gnn_embeddings.csv\n",
      "\n",
      "İşlem tamamlandı.\n",
      "Düğüm sayısı: 123\n",
      "\n",
      "GCN modeli çalıştırılıyor...\n",
      "Epoch 50/200, Loss: 1.3846\n",
      "Epoch 100/200, Loss: 1.3601\n",
      "Epoch 150/200, Loss: 1.3571\n",
      "Epoch 200/200, Loss: 1.3597\n",
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Score: 2359.6812)\n",
      "\n",
      "GCN_GNN Tamamlandı (48.95 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Embeddingler kaydedildi: ./ValidationWDE/community_results\\gcn_gnn_embeddings.csv\n",
      "\n",
      "GAT modeli çalıştırılıyor...\n",
      "Epoch 50/200, Loss: 19.5896\n",
      "Epoch 100/200, Loss: 26.4668\n",
      "Epoch 150/200, Loss: 26.7639\n",
      "Epoch 200/200, Loss: 21.1382\n",
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Score: 5497.9540)\n",
      "\n",
      "GAT_GNN Tamamlandı (56.13 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Embeddingler kaydedildi: ./ValidationWDE/community_results\\gat_gnn_embeddings.csv\n",
      "\n",
      "İşlem tamamlandı.\n",
      "Düğüm sayısı: 59\n",
      "\n",
      "GCN modeli çalıştırılıyor...\n",
      "Epoch 50/200, Loss: 1.3870\n",
      "Epoch 100/200, Loss: 1.3866\n",
      "Epoch 150/200, Loss: 1.3809\n",
      "Epoch 200/200, Loss: 1.3863\n",
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Beklenmedik bir hata: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)\n",
      "Düğüm sayısı: 59\n",
      "\n",
      "GCN modeli çalıştırılıyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kahra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (3). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kahra\\AppData\\Local\\Temp\\ipykernel_36044\\2548695269.py\", line 19, in <module>\n",
      "    communities, embeddings, elapsed_time, node_list = run_gnn_community_detection(\n",
      "  File \"C:\\Users\\kahra\\AppData\\Local\\Temp\\ipykernel_36044\\4111405331.py\", line 237, in run_gnn_community_detection\n",
      "    communities = cluster_embeddings(embeddings)\n",
      "  File \"C:\\Users\\kahra\\AppData\\Local\\Temp\\ipykernel_36044\\4111405331.py\", line 124, in cluster_embeddings\n",
      "    score = calinski_harabasz_score(embeddings_np, labels)\n",
      "  File \"C:\\Users\\kahra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 213, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\kahra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py\", line 373, in calinski_harabasz_score\n",
      "    check_number_of_labels(n_labels, n_samples)\n",
      "  File \"C:\\Users\\kahra\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py\", line 37, in check_number_of_labels\n",
      "    raise ValueError(\n",
      "ValueError: Number of labels is 1. Valid values are 2 to n_samples - 1 (inclusive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200, Loss: 1.3715\n",
      "Epoch 100/200, Loss: 1.3603\n",
      "Epoch 150/200, Loss: 1.3772\n",
      "Epoch 200/200, Loss: 1.3789\n",
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Score: 17596.0009)\n",
      "\n",
      "GCN_GNN Tamamlandı (22.23 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Embeddingler kaydedildi: ./TrainingWDE/community_results\\gcn_gnn_embeddings.csv\n",
      "\n",
      "GAT modeli çalıştırılıyor...\n",
      "Epoch 50/200, Loss: 21.2440\n",
      "Epoch 100/200, Loss: 25.2966\n",
      "Epoch 150/200, Loss: 25.0486\n",
      "Epoch 200/200, Loss: 18.7076\n",
      "En iyi küme sayısı aranıyor (3-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Score: 8414.4155)\n",
      "\n",
      "GAT_GNN Tamamlandı (26.46 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Embeddingler kaydedildi: ./TrainingWDE/community_results\\gat_gnn_embeddings.csv\n",
      "\n",
      "İşlem tamamlandı.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b635a421-253d-4954-ac4e-abff166cffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graf yükleniyor...\n",
      "\n",
      "============================================================\n",
      "İşleniyor: ./ValidationGeo\n",
      "============================================================\n",
      "Düğüm sayısı: 123\n",
      "Kenar sayısı: 3694\n",
      "\n",
      "GCN modeli çalıştırılıyor...\n",
      "Node feature'ları oluşturuluyor...\n",
      "Feature boyutu: 4\n",
      "Epoch 50/200, Loss: 0.9680\n",
      "Epoch 100/200, Loss: 0.9234\n",
      "Epoch 150/200, Loss: 0.8570\n",
      "Epoch 200/200, Loss: 0.8674\n",
      "Embedding istatistikleri - Mean: 0.0147, Std: 0.2670\n",
      "Benzersiz embedding sayısı: 121/123\n",
      "En iyi küme sayısı aranıyor (2-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Method: CH+Silhouette, Score: 775.8253)\n",
      "\n",
      "GCN_GNN Tamamlandı (50.64 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Küme büyüklükleri: Min=2, Max=15, Ortalama=6.2\n",
      "Embeddingler kaydedildi: ./ValidationGeo\\gcn_gnn_embeddings.csv\n",
      "Kümeler kaydedildi: ./ValidationGeo\\community_results\\gcn_gnn_communities.csv\n",
      "\n",
      "GAT modeli çalıştırılıyor...\n",
      "Node feature'ları oluşturuluyor...\n",
      "Feature boyutu: 4\n",
      "Epoch 50/200, Loss: 1.3164\n",
      "Epoch 100/200, Loss: 1.2058\n",
      "Epoch 150/200, Loss: 1.3498\n",
      "Epoch 200/200, Loss: 1.2858\n",
      "Embedding istatistikleri - Mean: -0.0179, Std: 0.1435\n",
      "Benzersiz embedding sayısı: 122/123\n",
      "En iyi küme sayısı aranıyor (2-20 arası)...\n",
      "Seçilen küme sayısı: 5 (Method: CH+Silhouette, Score: 723.1240)\n",
      "\n",
      "GAT_GNN Tamamlandı (55.19 sn)\n",
      "Toplam Küme Sayısı: 5\n",
      "Küme büyüklükleri: Min=14, Max=34, Ortalama=24.6\n",
      "Embeddingler kaydedildi: ./ValidationGeo\\gat_gnn_embeddings.csv\n",
      "Kümeler kaydedildi: ./ValidationGeo\\community_results\\gat_gnn_communities.csv\n",
      "\n",
      "./ValidationGeo için işlem tamamlandı.\n",
      "\n",
      "============================================================\n",
      "İşleniyor: ./ValidationWDE\n",
      "============================================================\n",
      "Düğüm sayısı: 123\n",
      "Kenar sayısı: 3694\n",
      "\n",
      "GCN modeli çalıştırılıyor...\n",
      "Node feature'ları oluşturuluyor...\n",
      "Feature boyutu: 4\n",
      "Epoch 50/200, Loss: 0.9637\n",
      "Epoch 100/200, Loss: 0.8802\n",
      "Epoch 150/200, Loss: 1.1150\n",
      "Epoch 200/200, Loss: 0.8713\n",
      "Embedding istatistikleri - Mean: 0.0260, Std: 0.2688\n",
      "Benzersiz embedding sayısı: 122/123\n",
      "En iyi küme sayısı aranıyor (2-20 arası)...\n",
      "Seçilen küme sayısı: 19 (Method: CH+Silhouette, Score: 680.0299)\n",
      "\n",
      "GCN_GNN Tamamlandı (51.97 sn)\n",
      "Toplam Küme Sayısı: 19\n",
      "Küme büyüklükleri: Min=2, Max=14, Ortalama=6.5\n",
      "Embeddingler kaydedildi: ./ValidationWDE\\gcn_gnn_embeddings.csv\n",
      "Kümeler kaydedildi: ./ValidationWDE\\community_results\\gcn_gnn_communities.csv\n",
      "\n",
      "GAT modeli çalıştırılıyor...\n",
      "Node feature'ları oluşturuluyor...\n",
      "Feature boyutu: 4\n",
      "Epoch 50/200, Loss: 1.2120\n",
      "Epoch 100/200, Loss: 1.3385\n",
      "Epoch 150/200, Loss: 1.1657\n",
      "Epoch 200/200, Loss: 1.2177\n",
      "Embedding istatistikleri - Mean: 0.0359, Std: 0.1406\n",
      "Benzersiz embedding sayısı: 122/123\n",
      "En iyi küme sayısı aranıyor (2-20 arası)...\n",
      "Seçilen küme sayısı: 18 (Method: CH+Silhouette, Score: 859.0875)\n",
      "\n",
      "GAT_GNN Tamamlandı (53.96 sn)\n",
      "Toplam Küme Sayısı: 18\n",
      "Küme büyüklükleri: Min=2, Max=16, Ortalama=6.8\n",
      "Embeddingler kaydedildi: ./ValidationWDE\\gat_gnn_embeddings.csv\n",
      "Kümeler kaydedildi: ./ValidationWDE\\community_results\\gat_gnn_communities.csv\n",
      "\n",
      "./ValidationWDE için işlem tamamlandı.\n",
      "\n",
      "============================================================\n",
      "İşleniyor: ./TrainingGeo\n",
      "============================================================\n",
      "Düğüm sayısı: 59\n",
      "Kenar sayısı: 1132\n",
      "\n",
      "GCN modeli çalıştırılıyor...\n",
      "Node feature'ları oluşturuluyor...\n",
      "Feature boyutu: 4\n",
      "Epoch 50/200, Loss: 1.2290\n",
      "Epoch 100/200, Loss: 1.1871\n",
      "Epoch 150/200, Loss: 1.1866\n",
      "Epoch 200/200, Loss: 1.2330\n",
      "Embedding istatistikleri - Mean: -0.0103, Std: 0.1477\n",
      "Benzersiz embedding sayısı: 48/59\n",
      "En iyi küme sayısı aranıyor (2-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Method: CH+Silhouette, Score: 2472.0447)\n",
      "\n",
      "GCN_GNN Tamamlandı (22.65 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Küme büyüklükleri: Min=1, Max=11, Ortalama=3.0\n",
      "Embeddingler kaydedildi: ./TrainingGeo\\gcn_gnn_embeddings.csv\n",
      "Kümeler kaydedildi: ./TrainingGeo\\community_results\\gcn_gnn_communities.csv\n",
      "\n",
      "GAT modeli çalıştırılıyor...\n",
      "Node feature'ları oluşturuluyor...\n",
      "Feature boyutu: 4\n",
      "Epoch 50/200, Loss: 1.3848\n",
      "Epoch 100/200, Loss: 1.3874\n",
      "Epoch 150/200, Loss: 1.3514\n",
      "Epoch 200/200, Loss: 1.3812\n",
      "Embedding istatistikleri - Mean: 0.0026, Std: 0.0592\n",
      "Benzersiz embedding sayısı: 48/59\n",
      "En iyi küme sayısı aranıyor (2-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Method: CH+Silhouette, Score: 1075.2735)\n",
      "\n",
      "GAT_GNN Tamamlandı (24.97 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Küme büyüklükleri: Min=1, Max=11, Ortalama=3.0\n",
      "Embeddingler kaydedildi: ./TrainingGeo\\gat_gnn_embeddings.csv\n",
      "Kümeler kaydedildi: ./TrainingGeo\\community_results\\gat_gnn_communities.csv\n",
      "\n",
      "./TrainingGeo için işlem tamamlandı.\n",
      "\n",
      "============================================================\n",
      "İşleniyor: ./TrainingWDE\n",
      "============================================================\n",
      "Düğüm sayısı: 59\n",
      "Kenar sayısı: 1132\n",
      "\n",
      "GCN modeli çalıştırılıyor...\n",
      "Node feature'ları oluşturuluyor...\n",
      "Feature boyutu: 4\n",
      "Epoch 50/200, Loss: 1.3462\n",
      "Epoch 100/200, Loss: 1.2692\n",
      "Epoch 150/200, Loss: 1.2090\n",
      "Epoch 200/200, Loss: 1.1936\n",
      "Embedding istatistikleri - Mean: 0.0070, Std: 0.1799\n",
      "Benzersiz embedding sayısı: 49/59\n",
      "En iyi küme sayısı aranıyor (2-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Method: CH+Silhouette, Score: 2982.0676)\n",
      "\n",
      "GCN_GNN Tamamlandı (22.32 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Küme büyüklükleri: Min=1, Max=11, Ortalama=3.0\n",
      "Embeddingler kaydedildi: ./TrainingWDE\\gcn_gnn_embeddings.csv\n",
      "Kümeler kaydedildi: ./TrainingWDE\\community_results\\gcn_gnn_communities.csv\n",
      "\n",
      "GAT modeli çalıştırılıyor...\n",
      "Node feature'ları oluşturuluyor...\n",
      "Feature boyutu: 4\n",
      "Epoch 50/200, Loss: 1.3134\n",
      "Epoch 100/200, Loss: 1.3835\n",
      "Epoch 150/200, Loss: 1.3767\n",
      "Epoch 200/200, Loss: 1.3853\n",
      "Embedding istatistikleri - Mean: -0.0072, Std: 0.0556\n",
      "Benzersiz embedding sayısı: 50/59\n",
      "En iyi küme sayısı aranıyor (2-20 arası)...\n",
      "Seçilen küme sayısı: 20 (Method: CH+Silhouette, Score: 1399.6287)\n",
      "\n",
      "GAT_GNN Tamamlandı (24.42 sn)\n",
      "Toplam Küme Sayısı: 20\n",
      "Küme büyüklükleri: Min=1, Max=11, Ortalama=3.0\n",
      "Embeddingler kaydedildi: ./TrainingWDE\\gat_gnn_embeddings.csv\n",
      "Kümeler kaydedildi: ./TrainingWDE\\community_results\\gat_gnn_communities.csv\n",
      "\n",
      "./TrainingWDE için işlem tamamlandı.\n",
      "\n",
      "============================================================\n",
      "TÜM İŞLEMLER TAMAMLANDI\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_networkx\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import calinski_harabasz_score, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# --- GCN Model Tanımı ---\n",
    "class GCN_Community(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, embedding_dim=32):\n",
    "        super(GCN_Community, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        # 1. Katman\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # 2. Katman\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # 3. Katman (Embedding Çıktısı)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# --- GAT Model Tanımı ---\n",
    "class GAT_Community(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_dim=64, embedding_dim=32, heads=4):\n",
    "        super(GAT_Community, self).__init__()\n",
    "        self.conv1 = GATConv(num_features, hidden_dim, heads=heads, dropout=0.6)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, embedding_dim, heads=1, concat=False, dropout=0.6)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# --- Negatif Örnekleme (Loss fonksiyonu için) ---\n",
    "def negative_sampling(data, num_neg_samples=None):\n",
    "    num_nodes = data.num_nodes\n",
    "    num_edges = data.edge_index.size(1)\n",
    "    if num_neg_samples is None:\n",
    "        num_neg_samples = num_edges\n",
    "    \n",
    "    neg_edges = []\n",
    "    edge_set = set(map(tuple, data.edge_index.t().tolist()))\n",
    "    \n",
    "    while len(neg_edges) < num_neg_samples:\n",
    "        i = np.random.randint(0, num_nodes)\n",
    "        j = np.random.randint(0, num_nodes)\n",
    "        if i != j and (i, j) not in edge_set and (j, i) not in edge_set:\n",
    "            neg_edges.append([i, j])\n",
    "    \n",
    "    return torch.tensor(neg_edges, dtype=torch.long).t().to(data.edge_index.device)\n",
    "\n",
    "# --- Eğitim Fonksiyonu ---\n",
    "def train_gnn(model, data, optimizer, epochs=200):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        embeddings = model(data)\n",
    "        \n",
    "        # Reconstruction Loss\n",
    "        pos_edge_index = data.edge_index\n",
    "        neg_edge_index = negative_sampling(data)\n",
    "        \n",
    "        # Pozitif kenarların skoru (birbirine yakın olmalı)\n",
    "        pos_loss = -torch.log(\n",
    "            torch.sigmoid(\n",
    "                (embeddings[pos_edge_index[0]] * embeddings[pos_edge_index[1]]).sum(dim=1)\n",
    "            ) + 1e-15\n",
    "        ).mean()\n",
    "        \n",
    "        # Negatif kenarların skoru (birbirine uzak olmalı)\n",
    "        neg_loss = -torch.log(\n",
    "            1 - torch.sigmoid(\n",
    "                (embeddings[neg_edge_index[0]] * embeddings[neg_edge_index[1]]).sum(dim=1)\n",
    "            ) + 1e-15\n",
    "        ).mean()\n",
    "        \n",
    "        loss = pos_loss + neg_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "# --- Geliştirilmiş Node Features ---\n",
    "def create_rich_node_features(G_nx):\n",
    "    \"\"\"\n",
    "    Daha zengin node feature'ları oluşturur:\n",
    "    - Degree\n",
    "    - Clustering coefficient\n",
    "    - PageRank\n",
    "    - Betweenness centrality (sadece küçük graflarda)\n",
    "    \"\"\"\n",
    "    node_list = list(G_nx.nodes())\n",
    "    features = []\n",
    "    \n",
    "    # Degree\n",
    "    degrees = dict(G_nx.degree())\n",
    "    \n",
    "    # Clustering coefficient\n",
    "    clustering = nx.clustering(G_nx)\n",
    "    \n",
    "    # PageRank\n",
    "    pagerank = nx.pagerank(G_nx, max_iter=100)\n",
    "    \n",
    "    # Betweenness (sadece küçük graflarda hesapla)\n",
    "    if G_nx.number_of_nodes() < 500:\n",
    "        betweenness = nx.betweenness_centrality(G_nx)\n",
    "    else:\n",
    "        betweenness = {node: 0.0 for node in node_list}\n",
    "    \n",
    "    for node in node_list:\n",
    "        features.append([\n",
    "            degrees[node],\n",
    "            clustering[node],\n",
    "            pagerank[node],\n",
    "            betweenness[node]\n",
    "        ])\n",
    "    \n",
    "    features_array = np.array(features, dtype=np.float32)\n",
    "    \n",
    "    # Normalize et\n",
    "    scaler = StandardScaler()\n",
    "    features_normalized = scaler.fit_transform(features_array)\n",
    "    \n",
    "    return torch.tensor(features_normalized, dtype=torch.float)\n",
    "\n",
    "# --- İyileştirilmiş Clustering ---\n",
    "def cluster_embeddings(embeddings, min_clusters=2, max_clusters=20):\n",
    "    \"\"\"\n",
    "    Geliştirilmiş kümeleme fonksiyonu:\n",
    "    - Embedding diversity kontrolü\n",
    "    - Silhouette score ile yedekleme\n",
    "    - Daha güvenli hata yönetimi\n",
    "    \"\"\"\n",
    "    # Tensor kontrolü\n",
    "    if torch.is_tensor(embeddings):\n",
    "        embeddings_np = embeddings.detach().cpu().numpy()\n",
    "    else:\n",
    "        embeddings_np = embeddings\n",
    "    \n",
    "    n_samples = len(embeddings_np)\n",
    "    \n",
    "    # Embedding çeşitliliğini kontrol et\n",
    "    unique_embeddings = np.unique(embeddings_np, axis=0)\n",
    "    print(f\"Benzersiz embedding sayısı: {len(unique_embeddings)}/{n_samples}\")\n",
    "    \n",
    "    if len(unique_embeddings) < min_clusters:\n",
    "        print(f\"⚠️ UYARI: Çok az benzersiz embedding ({len(unique_embeddings)}). Tüm noktalar tek kümede.\")\n",
    "        # Fallback: Tüm noktaları tek kümeye ata\n",
    "        return {0: list(range(n_samples))}\n",
    "    \n",
    "    # Küme sayısı limitlerini ayarla\n",
    "    max_possible_clusters = min(max_clusters, len(unique_embeddings) - 1, n_samples - 1)\n",
    "    min_clusters = max(2, min_clusters)\n",
    "    \n",
    "    if max_possible_clusters < min_clusters:\n",
    "        print(f\"⚠️ UYARI: Yeterli veri yok. Tüm noktalar tek kümede.\")\n",
    "        return {0: list(range(n_samples))}\n",
    "    \n",
    "    # Normalize embeddings\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings_np)\n",
    "    \n",
    "    best_score = -1\n",
    "    best_k = min_clusters\n",
    "    best_method = None\n",
    "    \n",
    "    print(f\"En iyi küme sayısı aranıyor ({min_clusters}-{max_possible_clusters} arası)...\")\n",
    "    \n",
    "    for k in range(min_clusters, max_possible_clusters + 1):\n",
    "        try:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=300)\n",
    "            labels = kmeans.fit_predict(embeddings_scaled)\n",
    "            \n",
    "            # Gerçekten k farklı küme var mı kontrol et\n",
    "            n_unique_labels = len(np.unique(labels))\n",
    "            \n",
    "            if n_unique_labels < 2:\n",
    "                continue\n",
    "            \n",
    "            # Calinski-Harabasz skorunu dene\n",
    "            try:\n",
    "                ch_score = calinski_harabasz_score(embeddings_scaled, labels)\n",
    "                \n",
    "                # Silhouette score ile doğrula\n",
    "                sil_score = silhouette_score(embeddings_scaled, labels)\n",
    "                \n",
    "                # Kombine skor (CH skoruna öncelik ver)\n",
    "                combined_score = ch_score * 0.7 + (sil_score + 1) * 1000 * 0.3\n",
    "                \n",
    "                if combined_score > best_score:\n",
    "                    best_score = combined_score\n",
    "                    best_k = k\n",
    "                    best_method = \"CH+Silhouette\"\n",
    "                    \n",
    "            except ValueError as e:\n",
    "                # CH skoru hesaplanamadıysa, sadece silhouette kullan\n",
    "                if n_unique_labels >= 2:\n",
    "                    sil_score = silhouette_score(embeddings_scaled, labels)\n",
    "                    if sil_score > best_score:\n",
    "                        best_score = sil_score\n",
    "                        best_k = k\n",
    "                        best_method = \"Silhouette\"\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"k={k} için hata: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Seçilen küme sayısı: {best_k} (Method: {best_method}, Score: {best_score:.4f})\")\n",
    "    \n",
    "    # Final kümeleme\n",
    "    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10, max_iter=300)\n",
    "    labels = kmeans.fit_predict(embeddings_scaled)\n",
    "    \n",
    "    communities = {}\n",
    "    for label in np.unique(labels):\n",
    "        communities[int(label)] = np.where(labels == label)[0].tolist()\n",
    "    \n",
    "    return communities\n",
    "\n",
    "# --- Embeddingleri Kaydetme Fonksiyonu (Path root'a kaydeder) ---\n",
    "def save_embeddings_to_csv(embeddings, node_list, algorithm_name, path):\n",
    "    \"\"\"\n",
    "    Embeddingleri path klasörünün içine kaydeder.\n",
    "    Format: NodeID, Emb_0, Emb_1, ..., Emb_N\n",
    "    \"\"\"\n",
    "    filename = os.path.join(path, f\"{algorithm_name.lower()}_embeddings.csv\")\n",
    "    \n",
    "    # Tensor kontrolü\n",
    "    if torch.is_tensor(embeddings):\n",
    "        embeddings_np = embeddings.detach().cpu().numpy()\n",
    "    else:\n",
    "        embeddings_np = embeddings\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            \n",
    "            # Header yaz (NodeID, dim0, dim1...)\n",
    "            header = [\"NodeID\"] + [f\"dim_{i}\" for i in range(embeddings_np.shape[1])]\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            # Her satırı yaz\n",
    "            for i, node_id in enumerate(node_list):\n",
    "                row = [node_id] + list(embeddings_np[i])\n",
    "                writer.writerow(row)\n",
    "                \n",
    "        print(f\"Embeddingler kaydedildi: {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding kaydetme hatası: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# --- Communities Kaydetme Fonksiyonu (community_results klasörüne kaydeder) ---\n",
    "def save_communities_to_csv(communities, node_list, algorithm_name, path):\n",
    "    \"\"\"\n",
    "    Kümeleri path/community_results klasörüne kaydeder.\n",
    "    \"\"\"\n",
    "    if not communities: \n",
    "        return False\n",
    "    \n",
    "    output_dir = os.path.join(path, \"community_results\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    filename = os.path.join(output_dir, f\"{algorithm_name.lower()}_communities.csv\")\n",
    "    \n",
    "    try:\n",
    "        with open(filename, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            for comm_id, member_indices in communities.items():\n",
    "                # İndeksleri gerçek düğüm isimlerine çevir\n",
    "                real_members = [node_list[idx] for idx in member_indices]\n",
    "                writer.writerow(real_members)\n",
    "        print(f\"Kümeler kaydedildi: {filename}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Community CSV hatası: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def run_gnn_community_detection(G_nx, model_type=\"gcn\", epochs=200, embedding_dim=32):\n",
    "    print(f\"\\n{model_type.upper()} modeli çalıştırılıyor...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Düğüm sırasını sabitle (NetworkX node order)\n",
    "    node_list = list(G_nx.nodes())\n",
    "    \n",
    "    data = from_networkx(G_nx)\n",
    "    \n",
    "    # Zengin node features oluştur\n",
    "    print(\"Node feature'ları oluşturuluyor...\")\n",
    "    data.x = create_rich_node_features(G_nx)\n",
    "    \n",
    "    num_features = data.x.size(1)\n",
    "    print(f\"Feature boyutu: {num_features}\")\n",
    "    \n",
    "    if model_type.lower() == \"gcn\":\n",
    "        model = GCN_Community(num_features, hidden_dim=64, embedding_dim=embedding_dim)\n",
    "    elif model_type.lower() == \"gat\":\n",
    "        model = GAT_Community(num_features, hidden_dim=64, embedding_dim=embedding_dim)\n",
    "    else:\n",
    "        print(f\"Bilinmeyen model: {model_type}\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    model = train_gnn(model, data, optimizer, epochs=epochs)\n",
    "    \n",
    "    # Embeddingleri al\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(data)\n",
    "    \n",
    "    # Embedding istatistikleri\n",
    "    emb_np = embeddings.detach().cpu().numpy()\n",
    "    print(f\"Embedding istatistikleri - Mean: {emb_np.mean():.4f}, Std: {emb_np.std():.4f}\")\n",
    "    \n",
    "    # Kümele\n",
    "    communities = cluster_embeddings(embeddings, min_clusters=2, max_clusters=20)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return communities, embeddings, elapsed_time, node_list\n",
    "\n",
    "\n",
    "# --- Ana Çalışma Döngüsü ---\n",
    "paths = [\"./ValidationGeo\", \"./ValidationWDE\", \"./TrainingGeo\", \"./TrainingWDE\"]\n",
    "print(\"Graf yükleniyor...\")\n",
    "\n",
    "for path in paths:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"İşleniyor: {path}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    try:\n",
    "        # Dosya adını kontrol et\n",
    "        adjlist_file = os.path.join(path, \"my.adjlist\")\n",
    "        if not os.path.exists(adjlist_file):\n",
    "            print(f\"HATA: '{adjlist_file}' dosyası bulunamadı!\")\n",
    "            continue\n",
    "    \n",
    "        G_nx = nx.read_adjlist(adjlist_file)\n",
    "        if nx.is_directed(G_nx):\n",
    "            G_nx = G_nx.to_undirected()\n",
    "            \n",
    "        print(f\"Düğüm sayısı: {G_nx.number_of_nodes()}\")\n",
    "        print(f\"Kenar sayısı: {G_nx.number_of_edges()}\")\n",
    "        \n",
    "        gnn_models = [\"gcn\", \"gat\"]\n",
    "        \n",
    "        for model_type in gnn_models:\n",
    "            communities, embeddings, elapsed_time, node_list = run_gnn_community_detection(\n",
    "                G_nx, \n",
    "                model_type=model_type,\n",
    "                epochs=200,\n",
    "                embedding_dim=32\n",
    "            )\n",
    "            \n",
    "            if communities:\n",
    "                algo_name = f\"{model_type.upper()}_GNN\"\n",
    "                \n",
    "                print(f\"\\n{algo_name} Tamamlandı ({elapsed_time:.2f} sn)\")\n",
    "                print(f\"Toplam Küme Sayısı: {len(communities)}\")\n",
    "                \n",
    "                # Küme büyüklüklerini göster\n",
    "                sizes = [len(members) for members in communities.values()]\n",
    "                print(f\"Küme büyüklükleri: Min={min(sizes)}, Max={max(sizes)}, Ortalama={np.mean(sizes):.1f}\")\n",
    "                \n",
    "                # 1. Embeddingleri path root'a kaydet\n",
    "                save_embeddings_to_csv(embeddings, node_list, algo_name, path)\n",
    "                \n",
    "                # 2. Kümeleri path/community_results içine kaydet\n",
    "                save_communities_to_csv(communities, node_list, algo_name, path)\n",
    "                \n",
    "        print(f\"\\n{path} için işlem tamamlandı.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Beklenmedik bir hata: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TÜM İŞLEMLER TAMAMLANDI\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5415399f-ab51-43ce-8a2a-2a6055b1d4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
